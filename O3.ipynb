{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "\n",
    "## Convolutional Neural Networks (CNNs)\n",
    "<details>\n",
    "For the journal: \n",
    "\n",
    "* write an introduction to CNNs (what are CNNs, what is a convolution layer, etc..), \n",
    "* document your experiments towards the end-goal of reaching 'a high accuracy' (what did you try, what work/did not work), \n",
    "* document how you use '_generalization_' in your setup (us of simple hold-out/train-test split or k-fold, or etc..),\n",
    "* produce some sort of '_learning-curve_' that illustrates the drop in cost- or increase in score-function with respect to, say training iteration (for inspiration see fig 4.20, 10-12 or 10.17 in [HOML]),\n",
    "* document the final CNN setup (layers etc., perhaps as a graph/drawing), \n",
    "* discus on your iterations towards the end-goal and other findings you had,\n",
    "* and, as always, write a conclusion.\n",
    "\n",
    "If you use a code template from slides, HOML or the internet, then remember to add a reference to the original work in you journal.\n",
    "</details>\n",
    "Excercise 9 from [HOML], p.496 2nd./p.535 3rd. (slighty modified):\n",
    "\n",
    "Convolational Neural NEtworks are categories as a deep learning model. It is use for processing grid-like data, such as images and series data. Now for explain the key components of CNN's. \n",
    "\n",
    "Convolutional layers are layers that apply learnable filters (kernels) to a small regions of the input data. They are not connect to all the data of example a image.\n",
    "\n",
    "Pooling Layer reducing the dimensionality, which can be reduce the amount of pixel. There are different pooling strategies maxpooling, avagere pooling. Maxpooling jsut takes the maximum value from a group of neighboring pixels to the input feature map. Avagere pooling is the sum \n",
    "\n",
    "Fully Connected Layers is after serveral convolution and pooling layers. There can be more than one of these layers and they act as a traditional neural network. Meaning they connect ever neuron from the previous layer to ever neuron in the next layer. \n",
    "\n",
    "Activation Functions is the non-linearity in the model. This can be ReLu, sigmoid or many more. It helps the model to learn complex relationships between features. \n",
    "\n",
    "Multiple Channels is often used with process images. Which allow to have multiple channels for each colour and for the outline of entity. The convolutional filters are applied independently to each channet. \n",
    "\n",
    "**Generalization**\n",
    "\n",
    "We are using a stratified k-fold cross-validation. K-fold is splitting the training set into k folds, then trainingthe model *k* times, holding out a different fold each time for evaluation. Stratified means maintenance the same propertions of classes as the orignal dataset when creating subsets or fold for training and validation. So the stratified k-fold performs strtified sampling to prodcue folds that contain a representative ratio of each class. With each iteration the code crates a clone of the classifer. With the clone we trains on the training folds and make prediction on the test fold. And then count the number of correct prediction and output the ratio of correct predictions. \n",
    "\n",
    "**Learning Curve** \n",
    "\n",
    "Tp \n",
    "\n",
    "__\"9. Build an CNN via the Keras API and try to achieve the highest possible accuracy on MNIST.\"__\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0646 - accuracy: 0.9865\n",
      "313/313 [==============================] - 2s 5ms/step - loss: 0.0487 - accuracy: 0.9907\n",
      "313/313 [==============================] - 2s 6ms/step - loss: 0.0452 - accuracy: 0.9896\n",
      "Test average accuracy: 0.9896000027656555\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "(X_train, y_train ), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "# Remove first element\n",
    "X_train = X_train.reshape((-1, 28, 28, 1)).astype('float32') / 255.0\n",
    "X_test = X_test.reshape((-1 , 28, 28, 1)).astype('float32') / 255.0\n",
    "\n",
    "y_train = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# 3, 3 means pixel... creates a 3x 3 pixels.\n",
    "# MaxPooling2D \n",
    "def createModel():\n",
    "    model = keras.models.Sequential()\n",
    "    model.add(keras.layers.Conv2D(32, (3, 3), padding=\"same\",activation='relu', input_shape=(28, 28 , 1)))\n",
    "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(keras.layers.MaxPooling2D((2, 2)))\n",
    "    model.add(keras.layers.Conv2D(64, (3, 3), padding=\"same\", activation='relu'))\n",
    "    model.add(keras.layers.Flatten())\n",
    "    model.add(keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "    model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "folds = 3\n",
    "kfold = StratifiedKFold(n_splits=folds, shuffle=True, random_state=42)\n",
    "test_accuracy = []\n",
    "\n",
    "for train, val in kfold.split(X_train, y_train.argmax(1)):\n",
    "\n",
    "    model = createModel()\n",
    "\n",
    "    X_train_kfold, x_val_kfold = X_train[train], X_train[val]\n",
    "    y_train_kfold, y_val_kfold = y_train[train], y_train[val]\n",
    "\n",
    "    model.fit(X_train_kfold, y_train_kfold, epochs=17, batch_size=64, verbose=0)\n",
    "\n",
    "    test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "\n",
    "avg_accuracy = np.mean(test_accuracy)\n",
    "print(f'Test average accuracy: {avg_accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "## Generalization Error\n",
    "<details><summary>Description</summary>\n",
    "In this exercise, we need to explain all important overall concepts in training. Let's begin with Figure 5.3 from Deep Learning (Ian Goodfellow, et. al. [DL]), that pretty much sums it all up\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L08/Figs/dl_generalization_error.png\" alt=\"WARNING: could not get image from server.\" style=\"height:500px\">\n",
    "</details>\n",
    "\n",
    "### Qa) On Generalization Error\n",
    "<details><summary>Description</summary>\n",
    "Write a detailed description of figure 5.3 (above) for your hand-in.\n",
    " \n",
    "All concepts in the figure must be explained \n",
    "\n",
    "* training/generalization error, \n",
    "* underfit/overfit zone, \n",
    "* optimal capacity, \n",
    "* generalization gab, \n",
    "* and the two axes: x/capacity, y/error.\n",
    "  </details>\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L08/Figs/dl_generalization_error.png\" alt=\"WARNING: could not get image from server.\" style=\"height:500px\">\n",
    "\n",
    "\n",
    "  We gave Chat-gpt a link to the graph and the Qa description to see if it could solve the assignment, however we have also explained the terms in our own words underneath. This is the result:\n",
    "\n",
    "<details><summary>ChatGTP answer</summary>\n",
    "  1. Training and Generalization Error:\n",
    "Training Error (Blue Curve): This curve represents the error (loss) on the training dataset as the capacity (complexity) of the model increases. Initially, the error decreases as the model becomes more complex, indicating better fit to the training data.\n",
    "\n",
    "Generalization Error (Red Curve): This curve represents the error (loss) on an unseen test/validation dataset. It measures how well the model performs on new, unseen data that it wasn't trained on.\n",
    "\n",
    "2. Underfit and Overfit Zone:\n",
    "Underfitting (Left Side): When the model's capacity is too low (left side of the graph), it struggles to capture the underlying patterns in the data. Both the training and generalization errors are high. This is a sign of insufficient complexity.\n",
    "\n",
    "Overfitting (Right Side): As the model's capacity increases, it starts to fit not only the underlying patterns but also the noise in the training data. This leads to a significant drop in training error but an increase in generalization error. The model is now fitting the noise, which is detrimental for unseen data.\n",
    "\n",
    "3. Optimal Capacity:\n",
    "Optimal Capacity (Middle Area): The sweet spot in the middle represents the optimal capacity of the model. It's the point where the generalization error is minimized. The model has enough complexity to capture the underlying patterns without overfitting to the noise.\n",
    "4. Generalization Gap:\n",
    "Generalization Gap (Vertical Distance): It's the vertical distance between the training error curve and the generalization error curve. It measures the difference in performance between the training and test datasets. A large gap indicates overfitting, while a small gap indicates good generalization.\n",
    "5. Axes: x/Capacity, y/Error:\n",
    "x-axis (Capacity): This represents the complexity or capacity of the model. It's a measure of how flexible the model is in capturing intricate patterns in the data.\n",
    "\n",
    "y-axis (Error): This axis shows the error (loss) associated with the model's predictions. Lower values indicate better performance, while higher values indicate poorer performance.\n",
    "\n",
    "In summary, Figure 5.3 illustrates the trade-off between the complexity of a model (capacity) and its ability to generalize to new, unseen data. The goal is to find the optimal capacity that minimizes the generalization error, striking a balance between underfitting and overfitting.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "On the graph a the y-axis describes the error amount, aka how often the model makes a wrong or partially incorrect prediction. The x-axis describes the capacity of the model, aka how capable or complex the model is. This can be increased by for example adding more neurons per layer, adding more total layers or switching to a more complex model.  \n",
    "\n",
    "**training error**\n",
    "The blue curve is training error. The training error curve describes the error percentage in relation to the capacity/complexity of the training model when used on the training data. Naturally when we increase the power of our model, the model should be able to understand and more patterns in the data. This is however not necnecessarily a good thing which will be elaborated in the overfitting segment.\n",
    "\n",
    "**Generalization error**\n",
    "The green curve is the Generalization error. The generalization error describes the error percentage in relation to the capacity/complexity of the training model when used on unseen test data. \n",
    "\n",
    "**Underfitting**\n",
    "Underfitting happens when a model is unable to understand enough of the patterns if any in the training and test data. This results in a higher training- and generalization error. If your model has a high generaliztion and tranning error it could be a sign that you have underfitted your model, and need to increase the complexity of the model. This can be done in many different ways.\n",
    "\n",
    "**Overfitting**\n",
    "Overfitting is when a too capable model is being used on a data set. This results in the model finding patterns in the \"noise\" of the training data that potntionally aren't relevant and therefore result in a higher generalization error. The Training error is typically very low since the very capable model has found all the patterns and more in the training data. If you experience a high generlization error and a very low training error it could be an indication that you need to reduce the capacity of your model, and the model has overcomplicated its understanding of the data. It could also be because there are patterns in the training data which are not present in the test data, so it would be a good idea to also check that you data-split correctly and mix the data.\n",
    "\n",
    "**Generalization gap**\n",
    "The generalization gap, is the vertical gap between the training error curve and the generalization curve. This means it describes the size of the difference between the errors that occur when the model is used on the training data and when the model is used on the test data.  It typically increases when the model has been overfitted.\n",
    "\n",
    "**Optimal Capacity**\n",
    "The optimal capacity is the perfect complexity for the model where it filters out the noise-patterns and picks up as many of the actual patterns as possible. This results in the training error generalization error and generalization gap being as low as possible. It is what you want to get as close to as possible when fitting you model. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb A MSE-Epoch/Error Plot\n",
    "<details><summary>Description</summary>\n",
    "Next, we look at a SGD model for fitting polynomial, that is _polynomial regression_ similar to what Géron describes in [HOML] (\"Polynomial Regression\" + \"Learning Curves\"). \n",
    "\n",
    "Review the code below for plotting the RMSE vs. the iteration number or epoch below (three cells, part I/II/III). \n",
    "\n",
    "Write a short description of the code, and comment on the important points in the generation of the (R)MSE array.\n",
    "\n",
    "The training phase output lots of lines like \n",
    "\n",
    "> `epoch= 104, mse_train=1.50, mse_val=2.37` <br>\n",
    "> `epoch= 105, mse_train=1.49, mse_val=2.35`\n",
    "\n",
    "What is an ___epoch___ and what is `mse_train` and `mse_val`?\n",
    "\n",
    "NOTE$_1$: the generalization plot figure 5.3 in [DL] (above) and the plots below have different x-axis, and are not to be compared directly!\n",
    "\n",
    "NOTE$_2$: notice that a 90 degree polynomial is used for the polynomial regression. This is just to produce a model with an extremly high capacity.\n",
    "</details>\n",
    "\n",
    "*Part 1:*\n",
    "\n",
    "The ```GenerateData()``` is used to generate random data points for ```X``` and add some noise to the random variable ```y``` to simulate realworld data. The `y` is a quadratic function, which is this mathimatical function f(x)=ax<sup>2</sup>+bx+c. After the the data is spilt into a training set and a validation set. \n",
    "\n",
    "After we create a pipeline which takes the PolynomialFeatures and StandardScaler. The \n",
    "After we create a pipeline which takes the PolynomialFeatures and StandardScaler. ```PolynomialFeatures``` is a prepocessor which have to inputs: the degree and the biased. We rotate the features 90 degree and don't include the bias. This is just to produce a model with an extremly high capacity. StandardScaler also a preprocesser, that standardized the featues in the dataset. This is done by removing the mean  and center the feature distribution around zero and after the scales the features variance. So this mean the variance of each feature will be 1.\n",
    "\n",
    "The we train and validation set are transformed with ```X_train_poly_scaled = poly_scaler.fit_transform(X_train)``` and ``` X_val_poly_scaled   = poly_scaler.transform(X_val)```\n",
    "\n",
    "\n",
    "*Part 2:*\n",
    " \n",
    "Creates a function call Train. It takes the inputs ```def Train(X_train, y_train, X_val, y_val, n_epochs, verbose=False)```. It makes a array of errors for both the validation and train. Then we run a SDGResgresster with only one iteration, where we have a constant learning rate. \n",
    "\n",
    "There after we have a for loop we run though each number of epochs, where we fit and predict for both train and validation. TTen we add the MSE for training and validation to the respective error array. if the verbose is set too true we print out each epoch and MSE for training and validation. \n",
    "\n",
    "> `epoch= 104, mse_train=1.50, mse_val=2.37` <br>\n",
    "The `epoch` is the number of training iteration. In each epoch train and validate the set and put it into a array. The `mse_train` is the mean squared error on the training set and it measures how well the it preform in this epoch. The same count for the `mse_val`. \n",
    "The output is use to help us monitor the training process and assess how well the model is learning form the data.\n",
    "\n",
    "\n",
    "*Part 3:*\n",
    "\n",
    "We find the best epoch using the minimum of our validate errors array. After that we calculate the RMSE for the best preform epoch. Then we plot out this epoch with a arrow. The plot is the epoch in the x-axis and RSME in the y-axis. A horizontal dotted line indicates the RMSE of the best model. The best model is the one with the lowest validation RMSE. This model will follow the underlying trend in the data without fitting too closely to the noise and is the best to unseen data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape= (25, 1)\n",
      "X_val  .shape= (25, 1)\n",
      "y_train.shape= (25,)\n",
      "y_val  .shape= (25,)\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Run code: Qb(part I)\n",
    "# NOTE: modified code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "def GenerateData():\n",
    "    m = 100\n",
    "    X = 6 * np.random.rand(m, 1) - 3\n",
    "    y = 2 + X + 0.5 * X**2 + np.random.randn(m, 1)\n",
    "    return X, y\n",
    "\n",
    "X, y = GenerateData()\n",
    "X_train, X_val, y_train, y_val = \\\n",
    "    train_test_split( \\\n",
    "        X[:50], y[:50].ravel(), \\\n",
    "        test_size=0.5, \\\n",
    "        random_state=10)\n",
    "\n",
    "print(\"X_train.shape=\",X_train.shape)\n",
    "print(\"X_val  .shape=\",X_val.shape)\n",
    "print(\"y_train.shape=\",y_train.shape)\n",
    "print(\"y_val  .shape=\",y_val.shape)\n",
    "\n",
    "poly_scaler = Pipeline([\n",
    "        (\"poly_features\", PolynomialFeatures(degree=90, include_bias=False)),\n",
    "        (\"std_scaler\", StandardScaler()),\n",
    "    ])\n",
    "\n",
    "X_train_poly_scaled = poly_scaler.fit_transform(X_train)\n",
    "X_val_poly_scaled   = poly_scaler.transform(X_val)\n",
    "\n",
    "X_new=np.linspace(-3, 3, 100).reshape(100, 1)\n",
    "plt.plot(X, y, \"b.\", label=\"All X-y Data\")\n",
    "plt.xlabel(\"$x_1$\", fontsize=18, )\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.legend(loc=\"upper left\", fontsize=14)\n",
    "plt.axis([-3, 3, 0, 10])\n",
    "plt.show()\n",
    "\n",
    "print('OK')      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...n_epochs= 500\n",
      "  epoch=   0, mse_train=11.85, mse_val=14.58\n",
      "  epoch=   1, mse_train=11.51, mse_val=14.10\n",
      "  epoch=   2, mse_train=11.15, mse_val=13.60\n",
      "  epoch=   3, mse_train=10.81, mse_val=13.13\n",
      "  epoch=   4, mse_train=10.49, mse_val=12.70\n",
      "  epoch=   5, mse_train=10.18, mse_val=12.30\n",
      "  epoch=   6, mse_train=9.88, mse_val=11.92\n",
      "  epoch=   7, mse_train=9.60, mse_val=11.56\n",
      "  epoch=   8, mse_train=9.33, mse_val=11.23\n",
      "  epoch=   9, mse_train=9.07, mse_val=10.91\n",
      "  epoch=  10, mse_train=8.82, mse_val=10.62\n",
      "  epoch=  11, mse_train=8.59, mse_val=10.34\n",
      "  epoch=  12, mse_train=8.36, mse_val=10.07\n",
      "  epoch=  13, mse_train=8.14, mse_val=9.82\n",
      "  epoch=  14, mse_train=7.93, mse_val=9.57\n",
      "  epoch=  15, mse_train=7.72, mse_val=9.34\n",
      "  epoch=  16, mse_train=7.53, mse_val=9.12\n",
      "  epoch=  17, mse_train=7.34, mse_val=8.91\n",
      "  epoch=  18, mse_train=7.16, mse_val=8.71\n",
      "  epoch=  19, mse_train=6.98, mse_val=8.52\n",
      "  epoch=  20, mse_train=6.81, mse_val=8.33\n",
      "  epoch=  21, mse_train=6.65, mse_val=8.15\n",
      "  epoch=  22, mse_train=6.49, mse_val=7.98\n",
      "  epoch=  23, mse_train=6.34, mse_val=7.81\n",
      "  epoch=  24, mse_train=6.19, mse_val=7.65\n",
      "  epoch=  25, mse_train=6.05, mse_val=7.49\n",
      "  epoch=  26, mse_train=5.91, mse_val=7.34\n",
      "  epoch=  27, mse_train=5.77, mse_val=7.20\n",
      "  epoch=  28, mse_train=5.64, mse_val=7.06\n",
      "  epoch=  29, mse_train=5.52, mse_val=6.92\n",
      "  epoch=  30, mse_train=5.40, mse_val=6.79\n",
      "  epoch=  31, mse_train=5.28, mse_val=6.66\n",
      "  epoch=  32, mse_train=5.16, mse_val=6.54\n",
      "  epoch=  33, mse_train=5.05, mse_val=6.42\n",
      "  epoch=  34, mse_train=4.94, mse_val=6.30\n",
      "  epoch=  35, mse_train=4.84, mse_val=6.18\n",
      "  epoch=  36, mse_train=4.73, mse_val=6.07\n",
      "  epoch=  37, mse_train=4.63, mse_val=5.97\n",
      "  epoch=  38, mse_train=4.54, mse_val=5.86\n",
      "  epoch=  39, mse_train=4.44, mse_val=5.76\n",
      "  epoch=  40, mse_train=4.35, mse_val=5.66\n",
      "  epoch=  41, mse_train=4.26, mse_val=5.56\n",
      "  epoch=  42, mse_train=4.17, mse_val=5.47\n",
      "  epoch=  43, mse_train=4.09, mse_val=5.37\n",
      "  epoch=  44, mse_train=4.01, mse_val=5.28\n",
      "  epoch=  45, mse_train=3.93, mse_val=5.20\n",
      "  epoch=  46, mse_train=3.85, mse_val=5.11\n",
      "  epoch=  47, mse_train=3.77, mse_val=5.03\n",
      "  epoch=  48, mse_train=3.70, mse_val=4.95\n",
      "  epoch=  49, mse_train=3.63, mse_val=4.87\n",
      "  epoch=  50, mse_train=3.56, mse_val=4.79\n",
      "  epoch=  51, mse_train=3.49, mse_val=4.71\n",
      "  epoch=  52, mse_train=3.42, mse_val=4.64\n",
      "  epoch=  53, mse_train=3.36, mse_val=4.57\n",
      "  epoch=  54, mse_train=3.29, mse_val=4.49\n",
      "  epoch=  55, mse_train=3.23, mse_val=4.43\n",
      "  epoch=  56, mse_train=3.17, mse_val=4.36\n",
      "  epoch=  57, mse_train=3.11, mse_val=4.29\n",
      "  epoch=  58, mse_train=3.06, mse_val=4.23\n",
      "  epoch=  59, mse_train=3.00, mse_val=4.16\n",
      "  epoch=  60, mse_train=2.95, mse_val=4.10\n",
      "  epoch=  61, mse_train=2.89, mse_val=4.04\n",
      "  epoch=  62, mse_train=2.84, mse_val=3.98\n",
      "  epoch=  63, mse_train=2.79, mse_val=3.93\n",
      "  epoch=  64, mse_train=2.74, mse_val=3.87\n",
      "  epoch=  65, mse_train=2.70, mse_val=3.81\n",
      "  epoch=  66, mse_train=2.65, mse_val=3.76\n",
      "  epoch=  67, mse_train=2.60, mse_val=3.71\n",
      "  epoch=  68, mse_train=2.56, mse_val=3.66\n",
      "  epoch=  69, mse_train=2.52, mse_val=3.60\n",
      "  epoch=  70, mse_train=2.47, mse_val=3.56\n",
      "  epoch=  71, mse_train=2.43, mse_val=3.51\n",
      "  epoch=  72, mse_train=2.39, mse_val=3.46\n",
      "  epoch=  73, mse_train=2.35, mse_val=3.41\n",
      "  epoch=  74, mse_train=2.31, mse_val=3.37\n",
      "  epoch=  75, mse_train=2.28, mse_val=3.32\n",
      "  epoch=  76, mse_train=2.24, mse_val=3.28\n",
      "  epoch=  77, mse_train=2.20, mse_val=3.24\n",
      "  epoch=  78, mse_train=2.17, mse_val=3.20\n",
      "  epoch=  79, mse_train=2.14, mse_val=3.15\n",
      "  epoch=  80, mse_train=2.10, mse_val=3.12\n",
      "  epoch=  81, mse_train=2.07, mse_val=3.08\n",
      "  epoch=  82, mse_train=2.04, mse_val=3.04\n",
      "  epoch=  83, mse_train=2.01, mse_val=3.00\n",
      "  epoch=  84, mse_train=1.98, mse_val=2.96\n",
      "  epoch=  85, mse_train=1.95, mse_val=2.93\n",
      "  epoch=  86, mse_train=1.92, mse_val=2.89\n",
      "  epoch=  87, mse_train=1.89, mse_val=2.86\n",
      "  epoch=  88, mse_train=1.86, mse_val=2.82\n",
      "  epoch=  89, mse_train=1.84, mse_val=2.79\n",
      "  epoch=  90, mse_train=1.81, mse_val=2.76\n",
      "  epoch=  91, mse_train=1.79, mse_val=2.73\n",
      "  epoch=  92, mse_train=1.76, mse_val=2.70\n",
      "  epoch=  93, mse_train=1.74, mse_val=2.67\n",
      "  epoch=  94, mse_train=1.71, mse_val=2.64\n",
      "  epoch=  95, mse_train=1.69, mse_val=2.61\n",
      "  epoch=  96, mse_train=1.67, mse_val=2.58\n",
      "  epoch=  97, mse_train=1.65, mse_val=2.55\n",
      "  epoch=  98, mse_train=1.62, mse_val=2.52\n",
      "  epoch=  99, mse_train=1.60, mse_val=2.50\n",
      "  epoch= 100, mse_train=1.58, mse_val=2.47\n",
      "  epoch= 101, mse_train=1.56, mse_val=2.45\n",
      "  epoch= 102, mse_train=1.54, mse_val=2.42\n",
      "  epoch= 103, mse_train=1.52, mse_val=2.40\n",
      "  epoch= 104, mse_train=1.50, mse_val=2.37\n",
      "  epoch= 105, mse_train=1.49, mse_val=2.35\n",
      "  epoch= 106, mse_train=1.47, mse_val=2.33\n",
      "  epoch= 107, mse_train=1.45, mse_val=2.30\n",
      "  epoch= 108, mse_train=1.43, mse_val=2.28\n",
      "  epoch= 109, mse_train=1.42, mse_val=2.26\n",
      "  epoch= 110, mse_train=1.40, mse_val=2.24\n",
      "  epoch= 111, mse_train=1.38, mse_val=2.22\n",
      "  epoch= 112, mse_train=1.37, mse_val=2.20\n",
      "  epoch= 113, mse_train=1.35, mse_val=2.18\n",
      "  epoch= 114, mse_train=1.34, mse_val=2.16\n",
      "  epoch= 115, mse_train=1.32, mse_val=2.14\n",
      "  epoch= 116, mse_train=1.31, mse_val=2.12\n",
      "  epoch= 117, mse_train=1.30, mse_val=2.10\n",
      "  epoch= 118, mse_train=1.28, mse_val=2.08\n",
      "  epoch= 119, mse_train=1.27, mse_val=2.06\n",
      "  epoch= 120, mse_train=1.26, mse_val=2.05\n",
      "  epoch= 121, mse_train=1.24, mse_val=2.03\n",
      "  epoch= 122, mse_train=1.23, mse_val=2.01\n",
      "  epoch= 123, mse_train=1.22, mse_val=2.00\n",
      "  epoch= 124, mse_train=1.21, mse_val=1.98\n",
      "  epoch= 125, mse_train=1.19, mse_val=1.97\n",
      "  epoch= 126, mse_train=1.18, mse_val=1.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch= 127, mse_train=1.17, mse_val=1.94\n",
      "  epoch= 128, mse_train=1.16, mse_val=1.92\n",
      "  epoch= 129, mse_train=1.15, mse_val=1.91\n",
      "  epoch= 130, mse_train=1.14, mse_val=1.89\n",
      "  epoch= 131, mse_train=1.13, mse_val=1.88\n",
      "  epoch= 132, mse_train=1.12, mse_val=1.87\n",
      "  epoch= 133, mse_train=1.11, mse_val=1.85\n",
      "  epoch= 134, mse_train=1.10, mse_val=1.84\n",
      "  epoch= 135, mse_train=1.09, mse_val=1.83\n",
      "  epoch= 136, mse_train=1.08, mse_val=1.81\n",
      "  epoch= 137, mse_train=1.07, mse_val=1.80\n",
      "  epoch= 138, mse_train=1.06, mse_val=1.79\n",
      "  epoch= 139, mse_train=1.06, mse_val=1.78\n",
      "  epoch= 140, mse_train=1.05, mse_val=1.77\n",
      "  epoch= 141, mse_train=1.04, mse_val=1.76\n",
      "  epoch= 142, mse_train=1.03, mse_val=1.74\n",
      "  epoch= 143, mse_train=1.02, mse_val=1.73\n",
      "  epoch= 144, mse_train=1.02, mse_val=1.72\n",
      "  epoch= 145, mse_train=1.01, mse_val=1.71\n",
      "  epoch= 146, mse_train=1.00, mse_val=1.70\n",
      "  epoch= 147, mse_train=0.99, mse_val=1.69\n",
      "  epoch= 148, mse_train=0.99, mse_val=1.68\n",
      "  epoch= 149, mse_train=0.98, mse_val=1.67\n",
      "  epoch= 150, mse_train=0.97, mse_val=1.67\n",
      "  epoch= 151, mse_train=0.97, mse_val=1.66\n",
      "  epoch= 152, mse_train=0.96, mse_val=1.65\n",
      "  epoch= 153, mse_train=0.95, mse_val=1.64\n",
      "  epoch= 154, mse_train=0.95, mse_val=1.63\n",
      "  epoch= 155, mse_train=0.94, mse_val=1.62\n",
      "  epoch= 156, mse_train=0.93, mse_val=1.61\n",
      "  epoch= 157, mse_train=0.93, mse_val=1.61\n",
      "  epoch= 158, mse_train=0.92, mse_val=1.60\n",
      "  epoch= 159, mse_train=0.92, mse_val=1.59\n",
      "  epoch= 160, mse_train=0.91, mse_val=1.58\n",
      "  epoch= 161, mse_train=0.91, mse_val=1.58\n",
      "  epoch= 162, mse_train=0.90, mse_val=1.57\n",
      "  epoch= 163, mse_train=0.90, mse_val=1.56\n",
      "  epoch= 164, mse_train=0.89, mse_val=1.56\n",
      "  epoch= 165, mse_train=0.89, mse_val=1.55\n",
      "  epoch= 166, mse_train=0.88, mse_val=1.54\n",
      "  epoch= 167, mse_train=0.88, mse_val=1.54\n",
      "  epoch= 168, mse_train=0.87, mse_val=1.53\n",
      "  epoch= 169, mse_train=0.87, mse_val=1.52\n",
      "  epoch= 170, mse_train=0.86, mse_val=1.52\n",
      "  epoch= 171, mse_train=0.86, mse_val=1.51\n",
      "  epoch= 172, mse_train=0.85, mse_val=1.51\n",
      "  epoch= 173, mse_train=0.85, mse_val=1.50\n",
      "  epoch= 174, mse_train=0.84, mse_val=1.50\n",
      "  epoch= 175, mse_train=0.84, mse_val=1.49\n",
      "  epoch= 176, mse_train=0.84, mse_val=1.49\n",
      "  epoch= 177, mse_train=0.83, mse_val=1.48\n",
      "  epoch= 178, mse_train=0.83, mse_val=1.48\n",
      "  epoch= 179, mse_train=0.82, mse_val=1.47\n",
      "  epoch= 180, mse_train=0.82, mse_val=1.47\n",
      "  epoch= 181, mse_train=0.82, mse_val=1.46\n",
      "  epoch= 182, mse_train=0.81, mse_val=1.46\n",
      "  epoch= 183, mse_train=0.81, mse_val=1.45\n",
      "  epoch= 184, mse_train=0.81, mse_val=1.45\n",
      "  epoch= 185, mse_train=0.80, mse_val=1.45\n",
      "  epoch= 186, mse_train=0.80, mse_val=1.44\n",
      "  epoch= 187, mse_train=0.80, mse_val=1.44\n",
      "  epoch= 188, mse_train=0.79, mse_val=1.43\n",
      "  epoch= 189, mse_train=0.79, mse_val=1.43\n",
      "  epoch= 190, mse_train=0.79, mse_val=1.43\n",
      "  epoch= 191, mse_train=0.78, mse_val=1.42\n",
      "  epoch= 192, mse_train=0.78, mse_val=1.42\n",
      "  epoch= 193, mse_train=0.78, mse_val=1.42\n",
      "  epoch= 194, mse_train=0.77, mse_val=1.41\n",
      "  epoch= 195, mse_train=0.77, mse_val=1.41\n",
      "  epoch= 196, mse_train=0.77, mse_val=1.41\n",
      "  epoch= 197, mse_train=0.77, mse_val=1.40\n",
      "  epoch= 198, mse_train=0.76, mse_val=1.40\n",
      "  epoch= 199, mse_train=0.76, mse_val=1.40\n",
      "  epoch= 200, mse_train=0.76, mse_val=1.40\n",
      "  epoch= 201, mse_train=0.75, mse_val=1.39\n",
      "  epoch= 202, mse_train=0.75, mse_val=1.39\n",
      "  epoch= 203, mse_train=0.75, mse_val=1.39\n",
      "  epoch= 204, mse_train=0.75, mse_val=1.39\n",
      "  epoch= 205, mse_train=0.74, mse_val=1.39\n",
      "  epoch= 206, mse_train=0.74, mse_val=1.38\n",
      "  epoch= 207, mse_train=0.74, mse_val=1.38\n",
      "  epoch= 208, mse_train=0.74, mse_val=1.38\n",
      "  epoch= 209, mse_train=0.73, mse_val=1.38\n",
      "  epoch= 210, mse_train=0.73, mse_val=1.38\n",
      "  epoch= 211, mse_train=0.73, mse_val=1.37\n",
      "  epoch= 212, mse_train=0.73, mse_val=1.37\n",
      "  epoch= 213, mse_train=0.73, mse_val=1.37\n",
      "  epoch= 214, mse_train=0.72, mse_val=1.37\n",
      "  epoch= 215, mse_train=0.72, mse_val=1.37\n",
      "  epoch= 216, mse_train=0.72, mse_val=1.37\n",
      "  epoch= 217, mse_train=0.72, mse_val=1.36\n",
      "  epoch= 218, mse_train=0.72, mse_val=1.36\n",
      "  epoch= 219, mse_train=0.71, mse_val=1.36\n",
      "  epoch= 220, mse_train=0.71, mse_val=1.36\n",
      "  epoch= 221, mse_train=0.71, mse_val=1.36\n",
      "  epoch= 222, mse_train=0.71, mse_val=1.36\n",
      "  epoch= 223, mse_train=0.71, mse_val=1.36\n",
      "  epoch= 224, mse_train=0.70, mse_val=1.36\n",
      "  epoch= 225, mse_train=0.70, mse_val=1.36\n",
      "  epoch= 226, mse_train=0.70, mse_val=1.36\n",
      "  epoch= 227, mse_train=0.70, mse_val=1.36\n",
      "  epoch= 228, mse_train=0.70, mse_val=1.35\n",
      "  epoch= 229, mse_train=0.70, mse_val=1.35\n",
      "  epoch= 230, mse_train=0.69, mse_val=1.35\n",
      "  epoch= 231, mse_train=0.69, mse_val=1.35\n",
      "  epoch= 232, mse_train=0.69, mse_val=1.35\n",
      "  epoch= 233, mse_train=0.69, mse_val=1.35\n",
      "  epoch= 234, mse_train=0.69, mse_val=1.35\n",
      "  epoch= 235, mse_train=0.69, mse_val=1.35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch= 236, mse_train=0.68, mse_val=1.35\n",
      "  epoch= 237, mse_train=0.68, mse_val=1.35\n",
      "  epoch= 238, mse_train=0.68, mse_val=1.35\n",
      "  epoch= 239, mse_train=0.68, mse_val=1.35\n",
      "  epoch= 240, mse_train=0.68, mse_val=1.35\n",
      "  epoch= 241, mse_train=0.68, mse_val=1.35\n",
      "  epoch= 242, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 243, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 244, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 245, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 246, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 247, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 248, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 249, mse_train=0.67, mse_val=1.35\n",
      "  epoch= 250, mse_train=0.66, mse_val=1.35\n",
      "  epoch= 251, mse_train=0.66, mse_val=1.35\n",
      "  epoch= 252, mse_train=0.66, mse_val=1.35\n",
      "  epoch= 253, mse_train=0.66, mse_val=1.36\n",
      "  epoch= 254, mse_train=0.66, mse_val=1.36\n",
      "  epoch= 255, mse_train=0.66, mse_val=1.36\n",
      "  epoch= 256, mse_train=0.66, mse_val=1.36\n",
      "  epoch= 257, mse_train=0.66, mse_val=1.36\n",
      "  epoch= 258, mse_train=0.65, mse_val=1.36\n",
      "  epoch= 259, mse_train=0.65, mse_val=1.36\n",
      "  epoch= 260, mse_train=0.65, mse_val=1.36\n",
      "  epoch= 261, mse_train=0.65, mse_val=1.36\n",
      "  epoch= 262, mse_train=0.65, mse_val=1.36\n",
      "  epoch= 263, mse_train=0.65, mse_val=1.36\n",
      "  epoch= 264, mse_train=0.65, mse_val=1.36\n",
      "  epoch= 265, mse_train=0.65, mse_val=1.37\n",
      "  epoch= 266, mse_train=0.65, mse_val=1.37\n",
      "  epoch= 267, mse_train=0.64, mse_val=1.37\n",
      "  epoch= 268, mse_train=0.64, mse_val=1.37\n",
      "  epoch= 269, mse_train=0.64, mse_val=1.37\n",
      "  epoch= 270, mse_train=0.64, mse_val=1.37\n",
      "  epoch= 271, mse_train=0.64, mse_val=1.37\n",
      "  epoch= 272, mse_train=0.64, mse_val=1.37\n",
      "  epoch= 273, mse_train=0.64, mse_val=1.37\n",
      "  epoch= 274, mse_train=0.64, mse_val=1.38\n",
      "  epoch= 275, mse_train=0.64, mse_val=1.38\n",
      "  epoch= 276, mse_train=0.64, mse_val=1.38\n",
      "  epoch= 277, mse_train=0.63, mse_val=1.38\n",
      "  epoch= 278, mse_train=0.63, mse_val=1.38\n",
      "  epoch= 279, mse_train=0.63, mse_val=1.38\n",
      "  epoch= 280, mse_train=0.63, mse_val=1.38\n",
      "  epoch= 281, mse_train=0.63, mse_val=1.39\n",
      "  epoch= 282, mse_train=0.63, mse_val=1.39\n",
      "  epoch= 283, mse_train=0.63, mse_val=1.39\n",
      "  epoch= 284, mse_train=0.63, mse_val=1.39\n",
      "  epoch= 285, mse_train=0.63, mse_val=1.39\n",
      "  epoch= 286, mse_train=0.63, mse_val=1.39\n",
      "  epoch= 287, mse_train=0.63, mse_val=1.40\n",
      "  epoch= 288, mse_train=0.62, mse_val=1.40\n",
      "  epoch= 289, mse_train=0.62, mse_val=1.40\n",
      "  epoch= 290, mse_train=0.62, mse_val=1.40\n",
      "  epoch= 291, mse_train=0.62, mse_val=1.40\n",
      "  epoch= 292, mse_train=0.62, mse_val=1.40\n",
      "  epoch= 293, mse_train=0.62, mse_val=1.41\n",
      "  epoch= 294, mse_train=0.62, mse_val=1.41\n",
      "  epoch= 295, mse_train=0.62, mse_val=1.41\n",
      "  epoch= 296, mse_train=0.62, mse_val=1.41\n",
      "  epoch= 297, mse_train=0.62, mse_val=1.41\n",
      "  epoch= 298, mse_train=0.62, mse_val=1.42\n",
      "  epoch= 299, mse_train=0.62, mse_val=1.42\n",
      "  epoch= 300, mse_train=0.61, mse_val=1.42\n",
      "  epoch= 301, mse_train=0.61, mse_val=1.42\n",
      "  epoch= 302, mse_train=0.61, mse_val=1.42\n",
      "  epoch= 303, mse_train=0.61, mse_val=1.43\n",
      "  epoch= 304, mse_train=0.61, mse_val=1.43\n",
      "  epoch= 305, mse_train=0.61, mse_val=1.43\n",
      "  epoch= 306, mse_train=0.61, mse_val=1.43\n",
      "  epoch= 307, mse_train=0.61, mse_val=1.43\n",
      "  epoch= 308, mse_train=0.61, mse_val=1.44\n",
      "  epoch= 309, mse_train=0.61, mse_val=1.44\n",
      "  epoch= 310, mse_train=0.61, mse_val=1.44\n",
      "  epoch= 311, mse_train=0.61, mse_val=1.44\n",
      "  epoch= 312, mse_train=0.61, mse_val=1.44\n",
      "  epoch= 313, mse_train=0.61, mse_val=1.45\n",
      "  epoch= 314, mse_train=0.60, mse_val=1.45\n",
      "  epoch= 315, mse_train=0.60, mse_val=1.45\n",
      "  epoch= 316, mse_train=0.60, mse_val=1.45\n",
      "  epoch= 317, mse_train=0.60, mse_val=1.46\n",
      "  epoch= 318, mse_train=0.60, mse_val=1.46\n",
      "  epoch= 319, mse_train=0.60, mse_val=1.46\n",
      "  epoch= 320, mse_train=0.60, mse_val=1.46\n",
      "  epoch= 321, mse_train=0.60, mse_val=1.47\n",
      "  epoch= 322, mse_train=0.60, mse_val=1.47\n",
      "  epoch= 323, mse_train=0.60, mse_val=1.47\n",
      "  epoch= 324, mse_train=0.60, mse_val=1.47\n",
      "  epoch= 325, mse_train=0.60, mse_val=1.48\n",
      "  epoch= 326, mse_train=0.60, mse_val=1.48\n",
      "  epoch= 327, mse_train=0.60, mse_val=1.48\n",
      "  epoch= 328, mse_train=0.60, mse_val=1.48\n",
      "  epoch= 329, mse_train=0.60, mse_val=1.49\n",
      "  epoch= 330, mse_train=0.59, mse_val=1.49\n",
      "  epoch= 331, mse_train=0.59, mse_val=1.49\n",
      "  epoch= 332, mse_train=0.59, mse_val=1.49\n",
      "  epoch= 333, mse_train=0.59, mse_val=1.50\n",
      "  epoch= 334, mse_train=0.59, mse_val=1.50\n",
      "  epoch= 335, mse_train=0.59, mse_val=1.50\n",
      "  epoch= 336, mse_train=0.59, mse_val=1.50\n",
      "  epoch= 337, mse_train=0.59, mse_val=1.51\n",
      "  epoch= 338, mse_train=0.59, mse_val=1.51\n",
      "  epoch= 339, mse_train=0.59, mse_val=1.51\n",
      "  epoch= 340, mse_train=0.59, mse_val=1.51\n",
      "  epoch= 341, mse_train=0.59, mse_val=1.52\n",
      "  epoch= 342, mse_train=0.59, mse_val=1.52\n",
      "  epoch= 343, mse_train=0.59, mse_val=1.52\n",
      "  epoch= 344, mse_train=0.59, mse_val=1.52\n",
      "  epoch= 345, mse_train=0.59, mse_val=1.53\n",
      "  epoch= 346, mse_train=0.59, mse_val=1.53\n",
      "  epoch= 347, mse_train=0.59, mse_val=1.53\n",
      "  epoch= 348, mse_train=0.58, mse_val=1.53\n",
      "  epoch= 349, mse_train=0.58, mse_val=1.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch= 350, mse_train=0.58, mse_val=1.54\n",
      "  epoch= 351, mse_train=0.58, mse_val=1.54\n",
      "  epoch= 352, mse_train=0.58, mse_val=1.55\n",
      "  epoch= 353, mse_train=0.58, mse_val=1.55\n",
      "  epoch= 354, mse_train=0.58, mse_val=1.55\n",
      "  epoch= 355, mse_train=0.58, mse_val=1.55\n",
      "  epoch= 356, mse_train=0.58, mse_val=1.56\n",
      "  epoch= 357, mse_train=0.58, mse_val=1.56\n",
      "  epoch= 358, mse_train=0.58, mse_val=1.56\n",
      "  epoch= 359, mse_train=0.58, mse_val=1.57\n",
      "  epoch= 360, mse_train=0.58, mse_val=1.57\n",
      "  epoch= 361, mse_train=0.58, mse_val=1.57\n",
      "  epoch= 362, mse_train=0.58, mse_val=1.57\n",
      "  epoch= 363, mse_train=0.58, mse_val=1.58\n",
      "  epoch= 364, mse_train=0.58, mse_val=1.58\n",
      "  epoch= 365, mse_train=0.58, mse_val=1.58\n",
      "  epoch= 366, mse_train=0.58, mse_val=1.59\n",
      "  epoch= 367, mse_train=0.58, mse_val=1.59\n",
      "  epoch= 368, mse_train=0.58, mse_val=1.59\n",
      "  epoch= 369, mse_train=0.58, mse_val=1.59\n",
      "  epoch= 370, mse_train=0.57, mse_val=1.60\n",
      "  epoch= 371, mse_train=0.57, mse_val=1.60\n",
      "  epoch= 372, mse_train=0.57, mse_val=1.60\n",
      "  epoch= 373, mse_train=0.57, mse_val=1.61\n",
      "  epoch= 374, mse_train=0.57, mse_val=1.61\n",
      "  epoch= 375, mse_train=0.57, mse_val=1.61\n",
      "  epoch= 376, mse_train=0.57, mse_val=1.61\n",
      "  epoch= 377, mse_train=0.57, mse_val=1.62\n",
      "  epoch= 378, mse_train=0.57, mse_val=1.62\n",
      "  epoch= 379, mse_train=0.57, mse_val=1.62\n",
      "  epoch= 380, mse_train=0.57, mse_val=1.63\n",
      "  epoch= 381, mse_train=0.57, mse_val=1.63\n",
      "  epoch= 382, mse_train=0.57, mse_val=1.63\n",
      "  epoch= 383, mse_train=0.57, mse_val=1.64\n",
      "  epoch= 384, mse_train=0.57, mse_val=1.64\n",
      "  epoch= 385, mse_train=0.57, mse_val=1.64\n",
      "  epoch= 386, mse_train=0.57, mse_val=1.64\n",
      "  epoch= 387, mse_train=0.57, mse_val=1.65\n",
      "  epoch= 388, mse_train=0.57, mse_val=1.65\n",
      "  epoch= 389, mse_train=0.57, mse_val=1.65\n",
      "  epoch= 390, mse_train=0.57, mse_val=1.66\n",
      "  epoch= 391, mse_train=0.57, mse_val=1.66\n",
      "  epoch= 392, mse_train=0.57, mse_val=1.66\n",
      "  epoch= 393, mse_train=0.57, mse_val=1.67\n",
      "  epoch= 394, mse_train=0.57, mse_val=1.67\n",
      "  epoch= 395, mse_train=0.56, mse_val=1.67\n",
      "  epoch= 396, mse_train=0.56, mse_val=1.67\n",
      "  epoch= 397, mse_train=0.56, mse_val=1.68\n",
      "  epoch= 398, mse_train=0.56, mse_val=1.68\n",
      "  epoch= 399, mse_train=0.56, mse_val=1.68\n",
      "  epoch= 400, mse_train=0.56, mse_val=1.69\n",
      "  epoch= 401, mse_train=0.56, mse_val=1.69\n",
      "  epoch= 402, mse_train=0.56, mse_val=1.69\n",
      "  epoch= 403, mse_train=0.56, mse_val=1.70\n",
      "  epoch= 404, mse_train=0.56, mse_val=1.70\n",
      "  epoch= 405, mse_train=0.56, mse_val=1.70\n",
      "  epoch= 406, mse_train=0.56, mse_val=1.70\n",
      "  epoch= 407, mse_train=0.56, mse_val=1.71\n",
      "  epoch= 408, mse_train=0.56, mse_val=1.71\n",
      "  epoch= 409, mse_train=0.56, mse_val=1.71\n",
      "  epoch= 410, mse_train=0.56, mse_val=1.72\n",
      "  epoch= 411, mse_train=0.56, mse_val=1.72\n",
      "  epoch= 412, mse_train=0.56, mse_val=1.72\n",
      "  epoch= 413, mse_train=0.56, mse_val=1.73\n",
      "  epoch= 414, mse_train=0.56, mse_val=1.73\n",
      "  epoch= 415, mse_train=0.56, mse_val=1.73\n",
      "  epoch= 416, mse_train=0.56, mse_val=1.74\n",
      "  epoch= 417, mse_train=0.56, mse_val=1.74\n",
      "  epoch= 418, mse_train=0.56, mse_val=1.74\n",
      "  epoch= 419, mse_train=0.56, mse_val=1.74\n",
      "  epoch= 420, mse_train=0.56, mse_val=1.75\n",
      "  epoch= 421, mse_train=0.56, mse_val=1.75\n",
      "  epoch= 422, mse_train=0.56, mse_val=1.75\n",
      "  epoch= 423, mse_train=0.56, mse_val=1.76\n",
      "  epoch= 424, mse_train=0.56, mse_val=1.76\n",
      "  epoch= 425, mse_train=0.55, mse_val=1.76\n",
      "  epoch= 426, mse_train=0.55, mse_val=1.77\n",
      "  epoch= 427, mse_train=0.55, mse_val=1.77\n",
      "  epoch= 428, mse_train=0.55, mse_val=1.77\n",
      "  epoch= 429, mse_train=0.55, mse_val=1.78\n",
      "  epoch= 430, mse_train=0.55, mse_val=1.78\n",
      "  epoch= 431, mse_train=0.55, mse_val=1.78\n",
      "  epoch= 432, mse_train=0.55, mse_val=1.78\n",
      "  epoch= 433, mse_train=0.55, mse_val=1.79\n",
      "  epoch= 434, mse_train=0.55, mse_val=1.79\n",
      "  epoch= 435, mse_train=0.55, mse_val=1.79\n",
      "  epoch= 436, mse_train=0.55, mse_val=1.80\n",
      "  epoch= 437, mse_train=0.55, mse_val=1.80\n",
      "  epoch= 438, mse_train=0.55, mse_val=1.80\n",
      "  epoch= 439, mse_train=0.55, mse_val=1.81\n",
      "  epoch= 440, mse_train=0.55, mse_val=1.81\n",
      "  epoch= 441, mse_train=0.55, mse_val=1.81\n",
      "  epoch= 442, mse_train=0.55, mse_val=1.82\n",
      "  epoch= 443, mse_train=0.55, mse_val=1.82\n",
      "  epoch= 444, mse_train=0.55, mse_val=1.82\n",
      "  epoch= 445, mse_train=0.55, mse_val=1.82\n",
      "  epoch= 446, mse_train=0.55, mse_val=1.83\n",
      "  epoch= 447, mse_train=0.55, mse_val=1.83\n",
      "  epoch= 448, mse_train=0.55, mse_val=1.83\n",
      "  epoch= 449, mse_train=0.55, mse_val=1.84\n",
      "  epoch= 450, mse_train=0.55, mse_val=1.84\n",
      "  epoch= 451, mse_train=0.55, mse_val=1.84\n",
      "  epoch= 452, mse_train=0.55, mse_val=1.85\n",
      "  epoch= 453, mse_train=0.55, mse_val=1.85\n",
      "  epoch= 454, mse_train=0.55, mse_val=1.85\n",
      "  epoch= 455, mse_train=0.55, mse_val=1.86\n",
      "  epoch= 456, mse_train=0.55, mse_val=1.86\n",
      "  epoch= 457, mse_train=0.55, mse_val=1.86\n",
      "  epoch= 458, mse_train=0.55, mse_val=1.86\n",
      "  epoch= 459, mse_train=0.55, mse_val=1.87\n",
      "  epoch= 460, mse_train=0.55, mse_val=1.87\n",
      "  epoch= 461, mse_train=0.55, mse_val=1.87\n",
      "  epoch= 462, mse_train=0.55, mse_val=1.88\n",
      "  epoch= 463, mse_train=0.54, mse_val=1.88\n",
      "  epoch= 464, mse_train=0.54, mse_val=1.88\n",
      "  epoch= 465, mse_train=0.54, mse_val=1.89\n",
      "  epoch= 466, mse_train=0.54, mse_val=1.89\n",
      "  epoch= 467, mse_train=0.54, mse_val=1.89\n",
      "  epoch= 468, mse_train=0.54, mse_val=1.89\n",
      "  epoch= 469, mse_train=0.54, mse_val=1.90\n",
      "  epoch= 470, mse_train=0.54, mse_val=1.90\n",
      "  epoch= 471, mse_train=0.54, mse_val=1.90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch= 472, mse_train=0.54, mse_val=1.91\n",
      "  epoch= 473, mse_train=0.54, mse_val=1.91\n",
      "  epoch= 474, mse_train=0.54, mse_val=1.91\n",
      "  epoch= 475, mse_train=0.54, mse_val=1.92\n",
      "  epoch= 476, mse_train=0.54, mse_val=1.92\n",
      "  epoch= 477, mse_train=0.54, mse_val=1.92\n",
      "  epoch= 478, mse_train=0.54, mse_val=1.92\n",
      "  epoch= 479, mse_train=0.54, mse_val=1.93\n",
      "  epoch= 480, mse_train=0.54, mse_val=1.93\n",
      "  epoch= 481, mse_train=0.54, mse_val=1.93\n",
      "  epoch= 482, mse_train=0.54, mse_val=1.94\n",
      "  epoch= 483, mse_train=0.54, mse_val=1.94\n",
      "  epoch= 484, mse_train=0.54, mse_val=1.94\n",
      "  epoch= 485, mse_train=0.54, mse_val=1.95\n",
      "  epoch= 486, mse_train=0.54, mse_val=1.95\n",
      "  epoch= 487, mse_train=0.54, mse_val=1.95\n",
      "  epoch= 488, mse_train=0.54, mse_val=1.95\n",
      "  epoch= 489, mse_train=0.54, mse_val=1.96\n",
      "  epoch= 490, mse_train=0.54, mse_val=1.96\n",
      "  epoch= 491, mse_train=0.54, mse_val=1.96\n",
      "  epoch= 492, mse_train=0.54, mse_val=1.97\n",
      "  epoch= 493, mse_train=0.54, mse_val=1.97\n",
      "  epoch= 494, mse_train=0.54, mse_val=1.97\n",
      "  epoch= 495, mse_train=0.54, mse_val=1.97\n",
      "  epoch= 496, mse_train=0.54, mse_val=1.98\n",
      "  epoch= 497, mse_train=0.54, mse_val=1.98\n",
      "  epoch= 498, mse_train=0.54, mse_val=1.98\n",
      "  epoch= 499, mse_train=0.54, mse_val=1.99\n",
      "OK\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\linear_model\\_stochastic_gradient.py:1561: ConvergenceWarning: Maximum number of iteration reached before convergence. Consider increasing max_iter to improve the fit.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Run code: Qb(part II)\n",
    "\n",
    "def Train(X_train, y_train, X_val, y_val, n_epochs, verbose=False):\n",
    "    print(\"Training...n_epochs=\",n_epochs)\n",
    "    \n",
    "    train_errors, val_errors = [], []\n",
    "    \n",
    "    sgd_reg = SGDRegressor(max_iter=1,\n",
    "                           penalty=None,\n",
    "                           eta0=0.0005,\n",
    "                           warm_start=True,\n",
    "                           early_stopping=False,\n",
    "                           learning_rate=\"constant\",\n",
    "                           tol=-float(0),\n",
    "                           random_state=42)\n",
    "\n",
    "    for epoch in range(n_epochs):\n",
    "        \n",
    "        sgd_reg.fit(X_train, y_train)\n",
    "        \n",
    "        y_train_predict = sgd_reg.predict(X_train)\n",
    "        y_val_predict   = sgd_reg.predict(X_val)\n",
    "\n",
    "        mse_train=mean_squared_error(y_train, y_train_predict)\n",
    "        mse_val  =mean_squared_error(y_val  , y_val_predict)\n",
    "\n",
    "        train_errors.append(mse_train)\n",
    "        val_errors  .append(mse_val)\n",
    "        if verbose:\n",
    "            print(f\"  epoch={epoch:4d}, mse_train={mse_train:4.2f}, mse_val={mse_val:4.2f}\")\n",
    "\n",
    "    return train_errors, val_errors\n",
    "\n",
    "n_epochs = 500\n",
    "train_errors, val_errors = Train(X_train_poly_scaled, y_train, X_val_poly_scaled, y_val, n_epochs, True)\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n"
     ]
    }
   ],
   "source": [
    "# Run code: Qb(part III)\n",
    "\n",
    "best_epoch = np.argmin(val_errors)\n",
    "best_val_rmse = np.sqrt(val_errors[best_epoch])\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.annotate('Best model',\n",
    "             xy=(best_epoch, best_val_rmse),\n",
    "             xytext=(best_epoch, best_val_rmse + 1),\n",
    "             ha=\"center\",\n",
    "             arrowprops=dict(facecolor='black', shrink=0.05),\n",
    "             fontsize=16,\n",
    "            )\n",
    "\n",
    "best_val_rmse -= 0.03  # just to make the graph look better\n",
    "plt.plot([0, n_epochs], [best_val_rmse, best_val_rmse], \"k:\", linewidth=2)\n",
    "plt.plot(np.sqrt(train_errors), \"b--\", linewidth=2, label=\"Training set\")\n",
    "plt.plot(np.sqrt(val_errors), \"g-\", linewidth=3, label=\"Validation set\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Epoch\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc)  Early Stopping\n",
    "<details><summary>Description</summary>\n",
    "How would you implement ___early stopping___, in the code above? \n",
    "\n",
    "Write an explanation of the early stopping concept...that is, just write some pseudo code that 'implements' the early stopping. \n",
    "\n",
    "OPTIONAL: also implement your early stopping pseudo code in Python, and get it to work with the code above (and not just flipping the hyperparameter to `early_stopping=True` on the `SGDRegressor`).\n",
    "</details>\n",
    "\n",
    "\n",
    "For implementing early_stopping i would before the for loop add two values. One for the model and one for the best model. Inside the for loop I would check if this iteration of validation error is better the the best validataion error. This means this iteration is less than the current best validation error. Then i would add a check when the value of the this iteration becomes bigger than the best value, then break.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_val_error = float('inf')\n",
    "best_model = None\n",
    "\n",
    "#For loop here\n",
    "    \n",
    "    if val_error < best_val_error:\n",
    "        best_val_error = val_error\n",
    "        best_model = clone(sgd_reg)\n",
    "    \n",
    "    # Check for early stopping condition\n",
    "    if val_error > best_val_error:\n",
    "        print(f\"Early stopping at epoch {epoch}\")\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qd) Explain the Polynomial RMSE-Capacity plot\n",
    "<details><summary>Description</summary>\n",
    "Now we revisit the concepts from `capacity_under_overfitting.ipynb` notebook and the polynomial fitting with a given capacity (polynomial degree).\n",
    "\n",
    "Peek into the cell below (code similar to what we saw in `capacity_under_overfitting.ipynb`), and explain the generated RMSE-Capacity plot. Why does the _training error_ keep dropping, while the _CV-error_ drops until around capacity 3, and then begin to rise again?\n",
    "\n",
    "What does the x-axis _Capacity_ and y-axis _RMSE_ represent?\n",
    "\n",
    "Try increasing the model capacity. What happens when you do plots for `degrees` larger than around 10? Relate this with what you found via Qa+b in `capacity_under_overfitting.ipynb`.\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run and review this code\n",
    "# NOTE: modified code from [GITHOML], 04_training_linear_models.ipynb\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "from math import sqrt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def true_fun(X):\n",
    "    return np.cos(1.5 * np.pi * X)\n",
    "\n",
    "def GenerateData():\n",
    "    n_samples = 30\n",
    "    #degrees = [1, 4, 15]\n",
    "    degrees = range(1,8)\n",
    "\n",
    "    X = np.sort(np.random.rand(n_samples))\n",
    "    y = true_fun(X) + np.random.randn(n_samples) * 0.1\n",
    "    return X, y, degrees\n",
    "\n",
    "np.random.seed(0)\n",
    "X, y, degrees  = GenerateData()\n",
    "\n",
    "print(\"Iterating...degrees=\",degrees)\n",
    "capacities, rmses_training, rmses_validation= [], [], []\n",
    "for i in range(len(degrees)):\n",
    "    d=degrees[i]\n",
    "    \n",
    "    polynomial_features = PolynomialFeatures(degree=d, include_bias=False)\n",
    "    \n",
    "    linear_regression = LinearRegression()\n",
    "    pipeline = Pipeline([\n",
    "            (\"polynomial_features\", polynomial_features),\n",
    "            (\"linear_regression\", linear_regression)\n",
    "        ])\n",
    "    \n",
    "    Z = X[:, np.newaxis]\n",
    "    pipeline.fit(Z, y)\n",
    "    \n",
    "    p = pipeline.predict(Z)\n",
    "    train_rms = mean_squared_error(y,p)\n",
    "\n",
    "    # Evaluate the models using crossvalidation\n",
    "    scores = cross_val_score(pipeline, Z, y, scoring=\"neg_mean_squared_error\", cv=10)\n",
    "    score_mean = -scores.mean()\n",
    "    \n",
    "    rmse_training=sqrt(train_rms)\n",
    "    rmse_validation=sqrt(score_mean)\n",
    "    \n",
    "    print(f\"  degree={d:4d}, rmse_training={rmse_training:4.2f}, rmse_cv={rmse_validation:4.2f}\")\n",
    "    \n",
    "    capacities      .append(d)\n",
    "    rmses_training  .append(rmse_training)\n",
    "    rmses_validation.append(rmse_validation)\n",
    "    \n",
    "plt.figure(figsize=(7,4))\n",
    "plt.plot(capacities, rmses_training,  \"b--\", linewidth=2, label=\"training RMSE\")\n",
    "plt.plot(capacities, rmses_validation,\"g-\",  linewidth=2, label=\"validation RMSE\")\n",
    "plt.legend(loc=\"upper right\", fontsize=14)\n",
    "plt.xlabel(\"Capacity\", fontsize=14)\n",
    "plt.ylabel(\"RMSE\", fontsize=14)\n",
    "plt.show()\n",
    "\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "<details>\n",
    "## Hyperparameters and Gridsearch \n",
    "\n",
    "When instantiating a Scikit-learn model in python most or all constructor parameters have _default_ values. These values are not part of the internal model and are hence called ___hyperparameters___---in contrast to _normal_ model parameters, for example the neuron weights, $\\mathbf w$, for an `MLP` model.\n",
    "\n",
    "### Manual Tuning Hyperparameters\n",
    "\n",
    "Below is an example of the python constructor for the support-vector classifier `sklearn.svm.SVC`, with say the `kernel` hyperparameter having the default value `'rbf'`. If you should choose, what would you set it to other than `'rbf'`? \n",
    "\n",
    "```python\n",
    "class sklearn.svm.SVC(\n",
    "    C=1.0, \n",
    "    kernel=’rbf’, \n",
    "    degree=3,\n",
    "    gamma=’auto_deprecated’, \n",
    "    coef0=0.0, \n",
    "    shrinking=True, \n",
    "    probability=False, \n",
    "    tol=0.001, \n",
    "    cache_size=200, \n",
    "    class_weight=None, \n",
    "    verbose=False, \n",
    "    max_iter=-1, \n",
    "    decision_function_shape=’ovr’, \n",
    "    random_state=None\n",
    "  )\n",
    "```  \n",
    "\n",
    "The default values might be a sensible general starting point, but for your data, you might want to optimize the hyperparameters to yield a better result. \n",
    "\n",
    "To be able to set `kernel` to a sensible value you need to go into the documentation for the `SVC` and understand what the kernel parameter represents, and what values it can be set to, and you need to understand the consequences of setting `kernel` to something different than the default...and the story repeats for every other hyperparameter!\n",
    "\n",
    "### Brute Force  Search\n",
    "\n",
    "An alternative to this structured, but time-consuming approach, is just to __brute-force__ a search of interesting hyperparameters, and  choose the 'best' parameters according to a fit-predict and some score, say 'f1'. \n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L09/Figs/gridsearch.png\"  alt=\"WARNING: could not get image from server.\"  style=\"width:350px\">\n",
    "<small><em>\n",
    "    <center> Conceptual graphical view of grid search for two distinct hyperparameters. </center> \n",
    "    <center> Notice that you would normally search hyperparameters like `alpha` with an exponential range, say [0.01, 0.1, 1, 10] or similar.</center>\n",
    "</em></small>\n",
    "\n",
    "Now, you just pick out some hyperparameters, that you figure are important, set them to a suitable range, say\n",
    "\n",
    "```python\n",
    "    'kernel':('linear', 'rbf'), \n",
    "    'C':[1, 10]\n",
    "```\n",
    "and fire up a full (grid) search on this hyperparameter set, that will try out all your specified combination of `kernel` and `C` for the model, and then prints the hyperparameter set with the highest score...\n",
    "\n",
    "The demo code below sets up some of our well known 'hello-world' data and then run a _grid search_ on a particular model, here a _support-vector classifier_ (SVC)\n",
    "\n",
    "Other models and datasets  ('mnist', 'iris', 'moon') can also be examined.\n",
    "</details>\n",
    "\n",
    "### Qa Explain GridSearchCV\n",
    "<details><summary>Description</summary>\n",
    "There are two code cells below: 1) function setup, 2) the actual grid-search.\n",
    "\n",
    "Review the code cells and write a __short__ summary. Mainly focus on __cell 2__, but dig into cell 1 if you find it interesting (notice the use of local-function, a nifty feature in python).\n",
    "  \n",
    "In detail, examine the lines:  \n",
    "  \n",
    "```python\n",
    "grid_tuned = GridSearchCV(model, tuning_parameters, ..\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "..\n",
    "FullReport(grid_tuned , X_test, y_test, time_gridsearch)\n",
    "```\n",
    "and write a short description of how the `GridSeachCV` works: explain how the search parameter set is created and the overall search mechanism is functioning (without going into too much detail).\n",
    "\n",
    "What role does the parameter `scoring='f1_micro'` play in the `GridSearchCV`, and what does `n_jobs=-1` mean? \n",
    "</details>\n",
    "\n",
    "Start with setting up the data with the function ```LoadAndSetUpData('iris')```, which load the dataset based on a string. It can load different data 'iris', 'moon', or 'mnist'. Inside the function it check the the data is valid and splits it into train and test set. \n",
    "\n",
    "Afterwards we set up the search parameters as a support vecotr machine classifier with the value ```gamma=0.001```. The gamma influence the decision boundary, so a lower gamma means it is more sensivtive to individual data points. \n",
    "\n",
    "We specifi some of the hyperparameter for the (full) grid search. Here we try out the specified combination of `kernel` and `C` for the model , and then prints the hyperparameter set with the highest score. `kernel` specifies that the grid search will use two different kernel types. The ``rbf` stand for radial basis function. `C` defines the values for the regularization parameter. The requlaization is used elude overfitting, because model learns the training data too well and struggles to generalize to new, unseen data.\n",
    "\n",
    "The setting `CV` stands for how many cross-validation. `VERBOSE` is used for `observe` the training progress for each epoch. `verbos=0` will show you nothing (silent).\n",
    "\n",
    "```GridSearchCV(model, tuning_parameters, cv=CV, scoring='f1_micro', verbose=VERBOSE, n_jobs=-1)``` creates a gridsearch object and inputs are the explaned parameter. It uses a f1_mirco as a score and it calculates the F1 score for each class, then computes the average, giving equal weight to each class. The last hyperparameter is uses all available processors for parallel computing (n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK(function setup)\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "from sklearn import svm\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV, train_test_split\n",
    "from sklearn.metrics import classification_report, f1_score\n",
    "from sklearn import datasets\n",
    "\n",
    "import sys,os\n",
    "sys.path.append(os.path.expanduser('../'))\n",
    "from libitmal import dataloaders as itmaldataloaders # Needed for load of iris, moon and mnist\n",
    "\n",
    "currmode=\"N/A\" # GLOBAL var!\n",
    "\n",
    "def SearchReport(model): \n",
    "    \n",
    "    def GetBestModelCTOR(model, best_params):\n",
    "        def GetParams(best_params):\n",
    "            ret_str=\"\"          \n",
    "            for key in sorted(best_params):\n",
    "                value = best_params[key]\n",
    "                temp_str = \"'\" if str(type(value))==\"<class 'str'>\" else \"\"\n",
    "                if len(ret_str)>0:\n",
    "                    ret_str += ','\n",
    "                ret_str += f'{key}={temp_str}{value}{temp_str}'  \n",
    "            return ret_str          \n",
    "        try:\n",
    "            param_str = GetParams(best_params)\n",
    "            return type(model).__name__ + '(' + param_str + ')' \n",
    "        except:\n",
    "            return \"N/A(1)\"\n",
    "        \n",
    "    print(\"\\nBest model set found on train set:\")\n",
    "    print()\n",
    "    print(f\"\\tbest parameters={model.best_params_}\")\n",
    "    print(f\"\\tbest '{model.scoring}' score={model.best_score_}\")\n",
    "    print(f\"\\tbest index={model.best_index_}\")\n",
    "    print()\n",
    "    print(f\"Best estimator CTOR:\")\n",
    "    print(f\"\\t{model.best_estimator_}\")\n",
    "    print()\n",
    "    try:\n",
    "        print(f\"Grid scores ('{model.scoring}') on development set:\")\n",
    "        means = model.cv_results_['mean_test_score']\n",
    "        stds  = model.cv_results_['std_test_score']\n",
    "        i=0\n",
    "        for mean, std, params in zip(means, stds, model.cv_results_['params']):\n",
    "            print(\"\\t[%2d]: %0.3f (+/-%0.03f) for %r\" % (i, mean, std * 2, params))\n",
    "            i += 1\n",
    "    except:\n",
    "        print(\"WARNING: the random search do not provide means/stds\")\n",
    "    \n",
    "    global currmode                \n",
    "    assert \"f1_micro\"==str(model.scoring), f\"come on, we need to fix the scoring to be able to compare model-fits! Your scoreing={str(model.scoring)}...remember to add scoring='f1_micro' to the search\"   \n",
    "    return f\"best: dat={currmode}, score={model.best_score_:0.5f}, model={GetBestModelCTOR(model.estimator,model.best_params_)}\", model.best_estimator_ \n",
    "\n",
    "def ClassificationReport(model, X_test, y_test, target_names=None):\n",
    "    assert X_test.shape[0]==y_test.shape[0]\n",
    "    print(\"\\nDetailed classification report:\")\n",
    "    print(\"\\tThe model is trained on the full development set.\")\n",
    "    print(\"\\tThe scores are computed on the full evaluation set.\")\n",
    "    print()\n",
    "    y_true, y_pred = y_test, model.predict(X_test)                 \n",
    "    print(classification_report(y_true, y_pred, target_names=target_names))\n",
    "    print()\n",
    "    \n",
    "def FullReport(model, X_test, y_test, t):\n",
    "    print(f\"SEARCH TIME: {t:0.2f} sec\")\n",
    "    beststr, bestmodel = SearchReport(model)\n",
    "    ClassificationReport(model, X_test, y_test)    \n",
    "    print(f\"CTOR for best model: {bestmodel}\\n\")\n",
    "    print(f\"{beststr}\\n\")\n",
    "    return beststr, bestmodel\n",
    "    \n",
    "def LoadAndSetupData(mode, test_size=0.3):\n",
    "    assert test_size>=0.0 and test_size<=1.0\n",
    "    \n",
    "    def ShapeToString(Z):\n",
    "        n = Z.ndim\n",
    "        s = \"(\"\n",
    "        for i in range(n):\n",
    "            s += f\"{Z.shape[i]:5d}\"\n",
    "            if i+1!=n:\n",
    "                s += \";\"\n",
    "        return s+\")\"\n",
    "\n",
    "    global currmode\n",
    "    currmode=mode\n",
    "    print(f\"DATA: {currmode}..\")\n",
    "    \n",
    "    if mode=='moon':\n",
    "        X, y = itmaldataloaders.MOON_GetDataSet(n_samples=5000, noise=0.2)\n",
    "        itmaldataloaders.MOON_Plot(X, y)\n",
    "    elif mode=='mnist':\n",
    "        X, y = itmaldataloaders.MNIST_GetDataSet(load_mode=0)\n",
    "        if X.ndim==3:\n",
    "            X=np.reshape(X, (X.shape[0], -1))\n",
    "    elif mode=='iris':\n",
    "        X, y = itmaldataloaders.IRIS_GetDataSet()\n",
    "    else:\n",
    "        raise ValueError(f\"could not load data for that particular mode='{mode}', only 'moon'/'mnist'/'iris' supported\")\n",
    "        \n",
    "    print(f'  org. data:  X.shape      ={ShapeToString(X)}, y.shape      ={ShapeToString(y)}')\n",
    "\n",
    "    assert X.ndim==2\n",
    "    assert X.shape[0]==y.shape[0]\n",
    "    assert y.ndim==1 or (y.ndim==2 and y.shape[1]==0)    \n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=test_size, random_state=0, shuffle=True\n",
    "    )\n",
    "    \n",
    "    print(f'  train data: X_train.shape={ShapeToString(X_train)}, y_train.shape={ShapeToString(y_train)}')\n",
    "    print(f'  test data:  X_test.shape ={ShapeToString(X_test)}, y_test.shape ={ShapeToString(y_test)}')\n",
    "    print()\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test\n",
    "\n",
    "def TryKerasImport(verbose=True):\n",
    "    \n",
    "    kerasok = True\n",
    "    try:\n",
    "        import keras as keras_try\n",
    "    except:\n",
    "        kerasok = False\n",
    "\n",
    "    tensorflowkerasok = True\n",
    "    try:\n",
    "        import tensorflow.keras as tensorflowkeras_try\n",
    "    except:\n",
    "        tensorflowkerasok = False\n",
    "        \n",
    "    ok = kerasok or tensorflowkerasok\n",
    "    \n",
    "    if not ok and verbose:\n",
    "        if not kerasok:\n",
    "            print(\"WARNING: importing 'keras' failed\", file=sys.stderr)\n",
    "        if not tensorflowkerasok:\n",
    "            print(\"WARNING: importing 'tensorflow.keras' failed\", file=sys.stderr)\n",
    "\n",
    "    return ok\n",
    "    \n",
    "print(f\"OK(function setup\" + (\"\" if TryKerasImport() else \", hope MNIST loads works because it seems you miss the installation of Keras or Tensorflow!\") + \")\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: iris..\n",
      "  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n",
      "  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n",
      "  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n",
      "\n",
      "SEARCH TIME: 2.60 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'C': 1, 'kernel': 'linear'}\n",
      "\tbest 'f1_micro' score=0.9714285714285715\n",
      "\tbest index=2\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSVC(C=1, gamma=0.001, kernel='linear')\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.962 (+/-0.093) for {'C': 0.1, 'kernel': 'linear'}\n",
      "\t[ 1]: 0.371 (+/-0.038) for {'C': 0.1, 'kernel': 'rbf'}\n",
      "\t[ 2]: 0.971 (+/-0.047) for {'C': 1, 'kernel': 'linear'}\n",
      "\t[ 3]: 0.695 (+/-0.047) for {'C': 1, 'kernel': 'rbf'}\n",
      "\t[ 4]: 0.952 (+/-0.085) for {'C': 10, 'kernel': 'linear'}\n",
      "\t[ 5]: 0.924 (+/-0.097) for {'C': 10, 'kernel': 'rbf'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.94      0.97        18\n",
      "           2       0.92      1.00      0.96        11\n",
      "\n",
      "    accuracy                           0.98        45\n",
      "   macro avg       0.97      0.98      0.98        45\n",
      "weighted avg       0.98      0.98      0.98        45\n",
      "\n",
      "\n",
      "CTOR for best model: SVC(C=1, gamma=0.001, kernel='linear')\n",
      "\n",
      "best: dat=iris, score=0.97143, model=SVC(C=1,kernel='linear')\n",
      "\n",
      "OK(grid-search)\n"
     ]
    }
   ],
   "source": [
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData(\n",
    "    'iris')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = svm.SVC(\n",
    "    gamma=0.001\n",
    ")  # NOTE: gamma=\"scale\" does *not work in older Scikit-learn frameworks,\n",
    "# FIX:  replace with model = svm.SVC(gamma=0.001)\n",
    "\n",
    "tuning_parameters = {\n",
    "    'kernel': ('linear', 'rbf'), \n",
    "    'C': [0.1, 1, 10]\n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "grid_tuned = GridSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "# Report result\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qb Hyperparameter Grid Search using an SDG classifier\n",
    "<details><summary>Description</summary>\n",
    "Now, replace the `svm.SVC` model with an `SGDClassifier` and a suitable set of the hyperparameters for that model.\n",
    "\n",
    "You need at least four or five different hyperparameters from the `SGDClassifier` in the search-space before it begins to take considerable compute time doing the full grid search.\n",
    "\n",
    "So, repeat the search with the `SGDClassifier`, and be sure to add enough hyperparameters to the grid-search, such that the search takes a considerable time to run, that is a couple of minutes or up to some hours..\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: iris..\n",
      "  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n",
      "  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n",
      "  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n",
      "\n",
      "SEARCH TIME: 0.11 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'alpha': 0.01, 'max_iter': 1000, 'penalty': 'l1'}\n",
      "\tbest 'f1_micro' score=0.9619047619047618\n",
      "\tbest index=8\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(alpha=0.01, penalty='l1', random_state=42)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.819 (+/-0.304) for {'alpha': 0.0001, 'max_iter': 1000, 'penalty': 'l1'}\n",
      "\t[ 1]: 0.895 (+/-0.111) for {'alpha': 0.0001, 'max_iter': 1000, 'penalty': 'l2'}\n",
      "\t[ 2]: 0.819 (+/-0.304) for {'alpha': 0.0001, 'max_iter': 2000, 'penalty': 'l1'}\n",
      "\t[ 3]: 0.895 (+/-0.111) for {'alpha': 0.0001, 'max_iter': 2000, 'penalty': 'l2'}\n",
      "\t[ 4]: 0.943 (+/-0.071) for {'alpha': 0.001, 'max_iter': 1000, 'penalty': 'l1'}\n",
      "\t[ 5]: 0.924 (+/-0.076) for {'alpha': 0.001, 'max_iter': 1000, 'penalty': 'l2'}\n",
      "\t[ 6]: 0.943 (+/-0.071) for {'alpha': 0.001, 'max_iter': 2000, 'penalty': 'l1'}\n",
      "\t[ 7]: 0.924 (+/-0.076) for {'alpha': 0.001, 'max_iter': 2000, 'penalty': 'l2'}\n",
      "\t[ 8]: 0.962 (+/-0.038) for {'alpha': 0.01, 'max_iter': 1000, 'penalty': 'l1'}\n",
      "\t[ 9]: 0.886 (+/-0.076) for {'alpha': 0.01, 'max_iter': 1000, 'penalty': 'l2'}\n",
      "\t[10]: 0.962 (+/-0.038) for {'alpha': 0.01, 'max_iter': 2000, 'penalty': 'l1'}\n",
      "\t[11]: 0.886 (+/-0.076) for {'alpha': 0.01, 'max_iter': 2000, 'penalty': 'l2'}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.83      0.91        18\n",
      "           2       0.79      1.00      0.88        11\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.94      0.93        45\n",
      "weighted avg       0.95      0.93      0.93        45\n",
      "\n",
      "\n",
      "CTOR for best model: SGDClassifier(alpha=0.01, penalty='l1', random_state=42)\n",
      "\n",
      "best: dat=iris, score=0.96190, model=SGDClassifier(alpha=0.01,max_iter=1000,penalty='l1')\n",
      "\n",
      "OK(grid-search)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "# https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html\n",
    "\n",
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData(\n",
    "    'iris')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = SGDClassifier(\n",
    "    loss='hinge',\n",
    "    random_state=42\n",
    ")  \n",
    "\n",
    "tuning_parameters = {\n",
    "    #'kernel': ('linear', 'rbf'), \n",
    "    #'C': [0.1, 1, 10],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [1000, 2000],\n",
    "    'penalty': ['l1', 'l2']\n",
    "    #'learning_rate': ['optimal','constant']   \n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "grid_tuned = GridSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "# Report result\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qc Hyperparameter Random  Search using an SDG classifier\n",
    "<details><summary>Description</summary>\n",
    "Now, add code to run a `RandomizedSearchCV` instead.\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L09/Figs/randomsearch.png\" alt=\"WARNING: could not get image from server.\"  style=\"width:350px\" >\n",
    "<small><em>\n",
    "    <center> Conceptual graphical view of randomized search for two distinct hyperparameters. </center> \n",
    "</em></small>\n",
    "\n",
    "Use these default parameters for the random search, similar to the default parameters for the grid search\n",
    "\n",
    "```python\n",
    "random_tuned = RandomizedSearchCV(\n",
    "    model, \n",
    "    tuning_parameters, \n",
    "    n_iter=20, \n",
    "    random_state=42, \n",
    "    cv=CV, \n",
    "    scoring='f1_micro', \n",
    "    verbose=VERBOSE, \n",
    "    n_jobs=-1\n",
    ")\n",
    "```\n",
    "\n",
    "but with the two new parameters, `n_iter` and `random_state` added. Since the search-type is now random, the `random_state` gives sense, but essential to random search is the new `n_tier` parameter.\n",
    "\n",
    "So: investigate the `n_iter` parameter...in code and write a conceptual explanation  in text.\n",
    "\n",
    "Comparison of time (seconds) to complete `GridSearch` versus `RandomizedSearchCV`, does not necessarily give any sense, if your grid search completes in a few seconds (as for the iris tiny-data). You need a search that runs for minutes, hours, or days.\n",
    "\n",
    "But you could compare the best-tuned parameter set and best scoring for the two methods. Is the random search best model close to the grid search?  \n",
    "<details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: iris..\n",
      "  org. data:  X.shape      =(  150;    4), y.shape      =(  150)\n",
      "  train data: X_train.shape=(  105;    4), y_train.shape=(  105)\n",
      "  test data:  X_test.shape =(   45;    4), y_test.shape =(   45)\n",
      "\n",
      "SEARCH TIME: 0.10 sec\n",
      "\n",
      "Best model set found on train set:\n",
      "\n",
      "\tbest parameters={'penalty': 'l1', 'max_iter': 2000, 'alpha': 0.01}\n",
      "\tbest 'f1_micro' score=0.9619047619047618\n",
      "\tbest index=1\n",
      "\n",
      "Best estimator CTOR:\n",
      "\tSGDClassifier(alpha=0.01, max_iter=2000, penalty='l1', random_state=42)\n",
      "\n",
      "Grid scores ('f1_micro') on development set:\n",
      "\t[ 0]: 0.886 (+/-0.076) for {'penalty': 'l2', 'max_iter': 1000, 'alpha': 0.01}\n",
      "\t[ 1]: 0.962 (+/-0.038) for {'penalty': 'l1', 'max_iter': 2000, 'alpha': 0.01}\n",
      "\t[ 2]: 0.943 (+/-0.071) for {'penalty': 'l1', 'max_iter': 1000, 'alpha': 0.001}\n",
      "\t[ 3]: 0.943 (+/-0.071) for {'penalty': 'l1', 'max_iter': 2000, 'alpha': 0.001}\n",
      "\t[ 4]: 0.924 (+/-0.076) for {'penalty': 'l2', 'max_iter': 1000, 'alpha': 0.001}\n",
      "\t[ 5]: 0.819 (+/-0.304) for {'penalty': 'l1', 'max_iter': 2000, 'alpha': 0.0001}\n",
      "\t[ 6]: 0.924 (+/-0.076) for {'penalty': 'l2', 'max_iter': 2000, 'alpha': 0.001}\n",
      "\t[ 7]: 0.895 (+/-0.111) for {'penalty': 'l2', 'max_iter': 2000, 'alpha': 0.0001}\n",
      "\t[ 8]: 0.895 (+/-0.111) for {'penalty': 'l2', 'max_iter': 1000, 'alpha': 0.0001}\n",
      "\t[ 9]: 0.962 (+/-0.038) for {'penalty': 'l1', 'max_iter': 1000, 'alpha': 0.01}\n",
      "\n",
      "Detailed classification report:\n",
      "\tThe model is trained on the full development set.\n",
      "\tThe scores are computed on the full evaluation set.\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00        16\n",
      "           1       1.00      0.83      0.91        18\n",
      "           2       0.79      1.00      0.88        11\n",
      "\n",
      "    accuracy                           0.93        45\n",
      "   macro avg       0.93      0.94      0.93        45\n",
      "weighted avg       0.95      0.93      0.93        45\n",
      "\n",
      "\n",
      "CTOR for best model: SGDClassifier(alpha=0.01, max_iter=2000, penalty='l1', random_state=42)\n",
      "\n",
      "best: dat=iris, score=0.96190, model=SGDClassifier(alpha=0.01,max_iter=2000,penalty='l1')\n",
      "\n",
      "OK(grid-search)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData(\n",
    "    'iris')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = SGDClassifier(\n",
    "    loss='hinge',\n",
    "    random_state=42\n",
    ")  \n",
    "\n",
    "tuning_parameters = {\n",
    "    #'kernel': ('linear', 'rbf'), \n",
    "    #'C': [0.1, 1, 10],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [1000, 2000],\n",
    "    'penalty': ['l1', 'l2']\n",
    "    #'learning_rate': ['optimal','constant']   \n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "grid_tuned = RandomizedSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "# Report result\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Qd MNIST Search Quest II\n",
    "<details><summary>Description</summary>\n",
    "Finally, a search-quest competition: __who can find the best model+hyperparameters for the MNIST dataset?__\n",
    "\n",
    "You change to the MNIST data by calling `LoadAndSetupData('mnist')`, and this is a completely other ball-game that the iris _tiny-data_: it's much larger (but still far from _big-data_)!\n",
    "\n",
    "* You might opt for the exhaustive grid search, or use the faster but-less optimal random search...your choice. \n",
    "\n",
    "* You are free to pick any classifier in Scikit-learn, even algorithms we have not discussed yet---__except Neural Networks and KNeighborsClassifier!__. \n",
    "\n",
    "* Keep the score function at `f1_micro`, otherwise, we will be comparing 'æbler og pærer'. \n",
    "\n",
    "* And, you may also want to scale your input data for some models to perform better.\n",
    "\n",
    "* __REMEMBER__, DO NOT USE any Neural Network models. This also means not to use any `Keras` or `Tensorflow` models...since they outperform most other models, and there are also too many examples on the internet to cut-and-paste from!\n",
    "\n",
    "Check your result by printing the first _return_ value from `FullReport()` \n",
    "```python \n",
    "b1, m1 = FullReport(random_tuned , X_test, y_test, time_randomsearch)\n",
    "print(b1)\n",
    "```\n",
    "that will display a result like\n",
    "```\n",
    "best: dat=mnist, score=0.90780, model=SGDClassifier(alpha=1.0,eta0=0.0001,learning_rate='invscaling')\n",
    "```\n",
    "and paste your currently best model into the message box, for ITMAL group 09 like\n",
    "```\n",
    "Grp09: best: dat=mnist, score=0.90780, model=SGDClassifier(alpha=1.0,eta0=0.0001,learning_rate='invscaling')\n",
    "\n",
    "Grp09: CTOR for best model: SGDClassifier(alpha=1.0, average=False, class_weight=None, early_stopping=False,\n",
    "              epsilon=0.1, eta0=0.0001, fit_intercept=True, l1_ratio=0.15,\n",
    "              learning_rate='invscaling', loss='hinge', max_iter=1000,\n",
    "              n_iter_no_change=5, n_jobs=None, penalty='l2', power_t=0.5,\n",
    "              random_state=None, shuffle=True, tol=0.001,\n",
    "              validation_fraction=0.1, verbose=0, warm_start=False)\n",
    "```\n",
    "              \n",
    "on Brightspace: \"L09: Regularisering, optimering og søgning\" | \"Qd MNIST Search Quest\"\n",
    "\n",
    "> https://itundervisning.ase.au.dk/itmal_quest/index.php\n",
    "\n",
    "and, check if your score (for MNIST) is better than the currently best score. Republish if you get a better score than your own previously best. Deadline for submission of scores is the same as the deadline for the O3 journal handin.\n",
    "\n",
    "Remember to provide an ITMAL group name manually, so we can identify a winner: the 1. st price is  cake! \n",
    "\n",
    "For the journal hand-in, report your progress in scoring choosing different models, hyperparameters to search and how you might need to preprocess your data...and note, that the journal will not be accepted unless it contains information about Your results published on the Brightspace 'Search Quest II' page!\n",
    "<details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATA: mnist..\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 1s 0us/step\n",
      "  org. data:  X.shape      =(70000;  784), y.shape      =(70000)\n",
      "  train data: X_train.shape=(49000;  784), y_train.shape=(49000)\n",
      "  test data:  X_test.shape =(21000;  784), y_test.shape =(21000)\n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Bjark\\source\\repos\\MAL\\mal12\\L09\\gridsearch.ipynb Cell 9\u001b[0m line \u001b[0;36m3\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L09/gridsearch.ipynb#X11sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m grid_tuned \u001b[39m=\u001b[39m RandomizedSearchCV(model,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L09/gridsearch.ipynb#X11sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m                           tuning_parameters,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L09/gridsearch.ipynb#X11sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m                           cv\u001b[39m=\u001b[39mCV,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L09/gridsearch.ipynb#X11sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m                           scoring\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mf1_micro\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L09/gridsearch.ipynb#X11sZmlsZQ%3D%3D?line=29'>30</a>\u001b[0m                           verbose\u001b[39m=\u001b[39mVERBOSE,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L09/gridsearch.ipynb#X11sZmlsZQ%3D%3D?line=30'>31</a>\u001b[0m                           n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L09/gridsearch.ipynb#X11sZmlsZQ%3D%3D?line=32'>33</a>\u001b[0m start \u001b[39m=\u001b[39m time()\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L09/gridsearch.ipynb#X11sZmlsZQ%3D%3D?line=33'>34</a>\u001b[0m grid_tuned\u001b[39m.\u001b[39;49mfit(X_train, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L09/gridsearch.ipynb#X11sZmlsZQ%3D%3D?line=34'>35</a>\u001b[0m t \u001b[39m=\u001b[39m time() \u001b[39m-\u001b[39m start\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L09/gridsearch.ipynb#X11sZmlsZQ%3D%3D?line=36'>37</a>\u001b[0m \u001b[39m# Report result\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\base.py:1152\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     estimator\u001b[39m.\u001b[39m_validate_params()\n\u001b[0;32m   1147\u001b[0m \u001b[39mwith\u001b[39;00m config_context(\n\u001b[0;32m   1148\u001b[0m     skip_parameter_validation\u001b[39m=\u001b[39m(\n\u001b[0;32m   1149\u001b[0m         prefer_skip_nested_validation \u001b[39mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1150\u001b[0m     )\n\u001b[0;32m   1151\u001b[0m ):\n\u001b[1;32m-> 1152\u001b[0m     \u001b[39mreturn\u001b[39;00m fit_method(estimator, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:898\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    892\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    893\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    894\u001b[0m     )\n\u001b[0;32m    896\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 898\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    900\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    901\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    902\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:1809\u001b[0m, in \u001b[0;36mRandomizedSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1807\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1808\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Search n_iter candidates from param_distributions\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1809\u001b[0m     evaluate_candidates(\n\u001b[0;32m   1810\u001b[0m         ParameterSampler(\n\u001b[0;32m   1811\u001b[0m             \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_distributions, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mn_iter, random_state\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrandom_state\n\u001b[0;32m   1812\u001b[0m         )\n\u001b[0;32m   1813\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:845\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    838\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    839\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    840\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    841\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    842\u001b[0m         )\n\u001b[0;32m    843\u001b[0m     )\n\u001b[1;32m--> 845\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    846\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    847\u001b[0m         clone(base_estimator),\n\u001b[0;32m    848\u001b[0m         X,\n\u001b[0;32m    849\u001b[0m         y,\n\u001b[0;32m    850\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    851\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    852\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    853\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    854\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    855\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    856\u001b[0m     )\n\u001b[0;32m    857\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    858\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    859\u001b[0m     )\n\u001b[0;32m    860\u001b[0m )\n\u001b[0;32m    862\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    863\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    864\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    865\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    866\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    867\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\sklearn\\utils\\parallel.py:65\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     60\u001b[0m config \u001b[39m=\u001b[39m get_config()\n\u001b[0;32m     61\u001b[0m iterable_with_config \u001b[39m=\u001b[39m (\n\u001b[0;32m     62\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     63\u001b[0m     \u001b[39mfor\u001b[39;00m delayed_func, args, kwargs \u001b[39min\u001b[39;00m iterable\n\u001b[0;32m     64\u001b[0m )\n\u001b[1;32m---> 65\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__call__\u001b[39;49m(iterable_with_config)\n",
      "File \u001b[1;32mc:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\joblib\\parallel.py:1952\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1946\u001b[0m \u001b[39m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[0;32m   1947\u001b[0m \u001b[39m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[0;32m   1948\u001b[0m \u001b[39m# reach the first `yield` statement. This starts the aynchronous\u001b[39;00m\n\u001b[0;32m   1949\u001b[0m \u001b[39m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[0;32m   1950\u001b[0m \u001b[39mnext\u001b[39m(output)\n\u001b[1;32m-> 1952\u001b[0m \u001b[39mreturn\u001b[39;00m output \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturn_generator \u001b[39melse\u001b[39;00m \u001b[39mlist\u001b[39m(output)\n",
      "File \u001b[1;32mc:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\joblib\\parallel.py:1595\u001b[0m, in \u001b[0;36mParallel._get_outputs\u001b[1;34m(self, iterator, pre_dispatch)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[39myield\u001b[39;00m\n\u001b[0;32m   1594\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend\u001b[39m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1595\u001b[0m         \u001b[39myield from\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retrieve()\n\u001b[0;32m   1597\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mGeneratorExit\u001b[39;00m:\n\u001b[0;32m   1598\u001b[0m     \u001b[39m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[0;32m   1599\u001b[0m     \u001b[39m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[0;32m   1600\u001b[0m     \u001b[39m# the user if necessary.\u001b[39;00m\n\u001b[0;32m   1601\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Bjark\\source\\repos\\MAL\\MAL_V11\\Lib\\site-packages\\joblib\\parallel.py:1707\u001b[0m, in \u001b[0;36mParallel._retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1702\u001b[0m \u001b[39m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[0;32m   1703\u001b[0m \u001b[39m# async callbacks to progress.\u001b[39;00m\n\u001b[0;32m   1704\u001b[0m \u001b[39mif\u001b[39;00m ((\u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m) \u001b[39mor\u001b[39;00m\n\u001b[0;32m   1705\u001b[0m     (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mget_status(\n\u001b[0;32m   1706\u001b[0m         timeout\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtimeout) \u001b[39m==\u001b[39m TASK_PENDING)):\n\u001b[1;32m-> 1707\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.01\u001b[39m)\n\u001b[0;32m   1708\u001b[0m     \u001b[39mcontinue\u001b[39;00m\n\u001b[0;32m   1710\u001b[0m \u001b[39m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[0;32m   1711\u001b[0m \u001b[39m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[0;32m   1712\u001b[0m \u001b[39m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "# Setup data\n",
    "X_train, X_test, y_train, y_test = LoadAndSetupData(\n",
    "    'mnist')  # 'iris', 'moon', or 'mnist'\n",
    "\n",
    "# Setup search parameters\n",
    "model = SGDClassifier(\n",
    "    loss='hinge',\n",
    "    random_state=42\n",
    ")  \n",
    "\n",
    "tuning_parameters = {\n",
    "    #'kernel': ('linear', 'rbf'), \n",
    "    #'C': [0.1, 1, 10],\n",
    "    'alpha': [0.0001, 0.001, 0.01],\n",
    "    'max_iter': [1000, 2000],\n",
    "    'penalty': ['l1', 'l2']\n",
    "    #'learning_rate': ['optimal','constant']   \n",
    "}\n",
    "\n",
    "CV = 5\n",
    "VERBOSE = 0\n",
    "\n",
    "# Run GridSearchCV for the model\n",
    "grid_tuned = RandomizedSearchCV(model,\n",
    "                          tuning_parameters,\n",
    "                          cv=CV,\n",
    "                          scoring='f1_micro',\n",
    "                          verbose=VERBOSE,\n",
    "                          n_jobs=-1)\n",
    "\n",
    "start = time()\n",
    "grid_tuned.fit(X_train, y_train)\n",
    "t = time() - start\n",
    "\n",
    "# Report result\n",
    "b0, m0 = FullReport(grid_tuned, X_test, y_test, t)\n",
    "print('OK(grid-search)')"
   ]
  },
  {
   "attachments": {
    "roboflow-2.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "6b5e8e8f",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "## Advanced CNN using Roboflow\n",
    "\n",
    "Read the web page 'How to Train YOLOv8 Object Detection on a Custom Dataset' \n",
    "\n",
    "> https://blog.roboflow.com/how-to-train-yolov8-on-a-custom-dataset/\n",
    "\n",
    "Then create your own Roboflow project, with some data of your choice found on the site using\n",
    "\n",
    "> https://universe.roboflow.com/\n",
    "\n",
    "and clone an existing project. The Roboflow 'pipeline'<a name=\"cite_ref-1\"></a>[<sup>[1]</sup>](#cite_note-1) includes many steps\n",
    "\n",
    "![roboflow-2.png](attachment:roboflow-2.png)\n",
    "\n",
    "here you only want to look into the _process_ and _train_ steps and take a peek into the _collect_, _organize_ and _labelling_ step, which is normally the most time-consuming part of any ML project.\n",
    "\n",
    "The current computer vision/ML projects ohn Roboflow include object detection, classification, instance segmentation and semantic segmentation, and you are free to choose between the different computer vision-related concepts, but object detection and classification would probably be the best choice for this journal (instance and semantic segmentation are highly complicated processes.)\n",
    "\n",
    "Train and test a model and make documentation of the process for the journal, using images, and learning graphs. etc. from the site. \n",
    "\n",
    "(Notice, that there is no obvious way of getting hands-on any code behind the 'pipeline' in Roboflow, if, say you want to elaborate on the Yolov models on your own computer.)\n",
    "\n",
    "<a name=\"cite_note-1\"></a>[1][^](#cite_ref-1)  <span style=\"font-family:'Courier New'\">https://2486075003-files.gitbook.io/~/files/v0/b/gitbook-x-prod.appspot.com/o/spaces%2F-M6S9nPJhEX9FYH6clfW%2Fuploads%2FfHpPTWNdCVR9qHQDeskF%2FScreen%20Shot%202022-08-24%20at%2012.35.36%20PM.png?alt=media&token=623927fe-3099-4ccd-8aaa-890bf5c0b03b</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f5db2ce1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: goto Roboflow, and create a CNN project..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adcff2e6",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    ":-|:-\n",
    "2023-04-10| CEF, initial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
