{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "\n",
    "## Supergruppe diskussion\n",
    "\n",
    "\n",
    "## § 2 \"End-to-End Machine Learning Project\" [HOML]\n",
    "\n",
    "Genlæs kapitel  § 2 og forbered mundtlig præsentation.\n",
    "\n",
    "## Forberedelse inden lektionen\n",
    "\n",
    "Een eller flere af gruppe medlemmer forbereder et mundtligt resume af § 2:\n",
    "\n",
    "* i skal kunne give et kort mundligt resume af hele § 2 til en anden gruppe (på nær, som nævnt, Create the Workspace og Download the Data),\n",
    "\n",
    "* resume holdes til koncept-plan, dvs. prøv at genfortælle, hvad de overordnede linier er i kapitlerne i [HOML].\n",
    "\n",
    "Lav et kort skriftlig resume af de enkelte underafsnit, ca. 5 til 20 liners tekst, se \"TODO\"-template herunder (MUST, til O2 aflevering).\n",
    "\n",
    "Kapitler (incl. underkapitler):\n",
    "\n",
    "* _Look at the Big Picture,_\n",
    "* _Get the Data,_\n",
    "* _Explore and Visualize the Data to Gain Insights,_ \n",
    "* _Prepare the Data for Machine Learning Algorithms,_\n",
    "* _Select and Train a Model,_\n",
    "* _Fine-Tune Your Model,_\n",
    "* _Launch, Monitor, and Maintain Your System,_\n",
    "* _Try It Out!._\n",
    "\n",
    "## På klassen\n",
    "\n",
    "Supergruppe [SG] resume af § 2 End-to-End, ca. 30 til 45 min.\n",
    "\n",
    "* en supergruppe [SG], sammensættes af to grupper [G], on-the-fly på klassen,\n",
    "\n",
    "* hver gruppe [G] forbereder og giver en anden gruppe [G] et mundtligt resume af § 2 til en anden gruppe,\n",
    "\n",
    "* tid: ca. 30 mim. sammenlagt, den ene grupper genfortæller første halvdel af § 2 i ca. 15 min., hvorefter den anden gruppe genfortæller resten i ca. 15 min."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resume: Look at the Big Picture\n",
    "\n",
    "You have to frame the problem, and figure out what you are trying to solve. What is the core issue? Is it a reggression or classification task? \n",
    "Furthermore you have to select performance messures. fx Accuracy isn't very good when evaluating a digit evaluator.\n",
    "finally you have to check your assumptions. If you assume it is a regression task, but find out it is converted to a classification task down stream. Then that really sucks.\n",
    "\n",
    "#### Resume: Get the Data\n",
    "\n",
    "This segment talks about how to use jupyter notebooks which is presumed to be the thing we should skip summarising. \n",
    "When you aquire your data you should take a look at a few elements to ensure you know what types of data you are dealing with. \n",
    "The method describe() or hist can also give som standard statistical analysis to help you get a feel for the data.\n",
    "At this stage you should also create you test set, by shuffling the data and setting about 20% aside for testing.\n",
    "It needs to be shuffled so the algorithm doesn't just figure out every number is larger than the last fx.\n",
    "You should seed you shuffling so you get the same one each time. Otherwise if you train an algorithm on the same set several times it will end up knowing the whole set.\n",
    "You also need to ensure you data is representative of the true data population, otherwise you should consider stratifying your dataset.\n",
    "\n",
    "#### Resume: Explore and Visualize the Data to Gain Insights,\n",
    "This segment is about data exploration and visualization to grain insigt in the data. We have to get a more in depth about the data, which only do for the traing sets. It is a good idea to visualize the data through heatmaps/histograms. You may need to set the opacity down or color different segment for finding the patterns. You will also have to look at the correlations between the different value, but only if the dataset is not too large. The correlation coefficient ranges -1 to 1 and if the correlation is close to -1 or 1 it tells there is correlation between the data. Zero mean there is no linear correlation. The last part of this segment is about experiment with attribute combinations. Some atrributes maybe don't make sence they stand alone, so maybe combine them.\n",
    "\n",
    "\n",
    "**CHATGTP REWROTE:**\n",
    "\n",
    "\n",
    "This section focuses on delving deeper into data exploration and visualization to gain meaningful insights. This process is specifically applied to the training sets. Utilizing techniques like heatmaps and histograms proves beneficial in visualizing the data. Adjusting opacity or employing distinct colors aids in identifying underlying patterns. Additionally, assessing correlations between various values is advisable, particularly for datasets of manageable size. The correlation coefficient, ranging from -1 to 1, signifies the strength of correlation. Values closer to -1 or 1 indicate a significant correlation, while a value of zero denotes no linear correlation. The concluding part of this section underscores the importance of experimenting with attribute combinations. Some attributes may lack meaningful context in isolation, thus amalgamating them can yield more informative insights.\n",
    "#### Resume: Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "You need to find a strategy for handling missing datapoints such as entries missing some attributes. You can either remove them, use the median, or if you want to get fancy use the k-nearest neighbour.\n",
    "Furthermore machine learning models prefer numbers, and tend to skew towards numbers with larger intervals. Therfore it could be a good idea to standerdize your data to be between -1 - 1. This can be done by scaling all the \n",
    "\n",
    "#### Resume: Select and Train a Model\n",
    "\n",
    "You are now ready to select and train a model. Firstly try a simple model as linear regression, however some values may be way off. These data can be hard to understand, so we are going to use some tools for validation. This is can be MAE or RMSE. Another problem with the data if examples used the k-nearstnieghbor is overfitting or underfitting the training data. The main way to fix underfitting is try using a more complex model and how it proforms. The problem can also be the data is too small and one way to help simulate more date is it use the k-fold cross-validation, it spilt the data into nonoverlapping subsets. The model can now traing for every subsets and all the different score for each subsets put together give a value for its preformance. \n",
    "\n",
    "**CHAT V1**\n",
    "Now, you're prepared to choose and train a model. Begin with a simple model like linear regression, though be aware that some predictions may deviate significantly. To assess the quality of these predictions, employ validation tools such as MAE or RMSE. Another challenge with the data arises if, for instance, k-nearest neighbors lead to overfitting or underfitting of the training data. To address underfitting, consider experimenting with a more intricate model and evaluating its performance. Alternatively, if the dataset is limited in size, k-fold cross-validation can be employed to simulate additional data by partitioning it into distinct, non-overlapping subsets. Training the model on each subset and aggregating the scores yields an overall performance metric.\n",
    "\n",
    "**CHAT V2**\n",
    "You are now at the stage of model selection and training. To begin, consider implementing a straightforward model like linear regression. However, be prepared for potential outliers in the data that might lead to challenges in understanding the model's performance. To address this, validation tools such as Mean Absolute Error (MAE) or Root Mean Square Error (RMSE) can be employed.\n",
    "\n",
    "Another issue to watch for when dealing with this data is the risk of overfitting or underfitting when applying methods like k-nearest neighbors. To combat underfitting, experimenting with more complex models is advisable to observe how they perform.\n",
    "\n",
    "In cases where data size might be a limiting factor, one approach to simulate more data is by using k-fold cross-validation. This technique involves partitioning the data into non-overlapping subsets. The model is then trained on each of these subsets, and the scores obtained from these different subsets are aggregated to provide an overall performance evaluation.\n",
    "\n",
    "#### Resume: Fine-Tune Your Model\n",
    "\n",
    "After training the models, we have to fine tune the. For finetuning the model, you can manually try to find a great combination for the hyperparameter. Or use the GridSearchCV if you have a few hyperparameter or the RandomizedSearchCV for testing alot of hyperparameter. \n",
    "\n",
    "Another way to fine-tune your system is combine the models that performs best as a group. This mothod is called the Ensemble. It will often preform better the individual models. \n",
    "\n",
    "You may need to look into your best models and understand their errors. It can be some of the features don't make sense to have, so maybe you want to drop them. Or maybe it is because you are missing some features and need more data. May need to look into outliers.  \n",
    "\n",
    "When you system if preforming well on the train set, it may be time to try evaluating your system on the test set. After testing you will need to evalute the scores. The system may proferm really good on the test set or not. If it doesn't you may need to go back and finetune even more. If the system preform very well, then may need to make it ready for launch. \n",
    "\n",
    "**CHAT**\n",
    "After training the models, the next step is fine-tuning. This can be done manually by experimenting with hyperparameters, or using tools like GridSearchCV for a few hyperparameters or RandomizedSearchCV for a broader range. Another effective method is Ensemble, combining top-performing models to achieve even better results collectively.\n",
    "\n",
    "It's cruial to scrutinize the best models and understand their errors. This may involve removing irrelevant features, acquiring additional data, or addressing outliers. Once the system performs well on the training set, it's time to evaluate it on the test set. The results will determine if further fine-tuning is necessary. If the system excels, preparations for launch can commence.\n",
    "\n",
    "#### Resume: Launch, Monitor, and Maintain Your System\n",
    "\n",
    "If your system came to this step, you will need to get it ready for production. For a system to go into it need to be launch, monitor and maintain. This can be done differenly ways. First way is that you can create documentation and conduct tests and integrate it. The alternative approach is using a model on cloud platform. \n",
    "\n",
    "It is not the last step to just deploying it. You will have to collect new fresh data and keep training the model and evaluating performance. It is import to catch potiontial issuses early. It is also a good idea to have a backup of the models in a database. The models can end up being corrupted. \n",
    "\n",
    "#### Resume: Try It Out!.\n",
    "\n",
    "Now you have read all the a both segment, you will have a understanding of what machine learning project look like. You can now see how much goes into making a grate system. The machine learning algorigthms are important, however it is preferable to be comfortable with the hole process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "REVISIONS||\n",
    "---------||\n",
    "2019-01-28| CEF, initial.\n",
    "2020-02-05| CEF, F20 ITMAL update.\n",
    "2021-08-17| CEF, E21 ITMAL update.\n",
    "2021-09-17| CEF, corrected some spell errors.\n",
    "2022-01-28| CEF, update to F22 SWMAL.\n",
    "2022-09-09| CEF, corrected 'MUST for O1' to 'MUST for O2' in text.\n",
    "2023-02-13| CEF, updated to HOML 3rd, removed exclude subsections in 'Get the Data' in this excercise, since the parts with python environments has been removed in HOML."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
