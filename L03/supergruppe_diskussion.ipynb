{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "\n",
    "## Supergruppe diskussion\n",
    "\n",
    "\n",
    "## ยง 2 \"End-to-End Machine Learning Project\" [HOML]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Resume: Look at the Big Picture\n",
    "\n",
    "You have to frame the problem, and figure out what you are trying to solve. What is the core issue? Is it a reggression or classification task? \n",
    "Furthermore you have to select performance messures. fx Accuracy isn't very good when evaluating a digit evaluator.\n",
    "finally you have to check your assumptions. If you assume it is a regression task, but find out it is converted to a classification task down stream. Then that really sucks.\n",
    "\n",
    "#### Resume: Get the Data\n",
    "\n",
    "This segment talks about how to use jupyter notebooks which is presumed to be the thing we should skip summarising. \n",
    "When you aquire your data you should take a look at a few elements to ensure you know what types of data you are dealing with. \n",
    "The method describe() or hist can also give som standard statistical analysis to help you get a feel for the data.\n",
    "At this stage you should also create you test set, by shuffling the data and setting about 20% aside for testing.\n",
    "It needs to be shuffled so the algorithm doesn't just figure out every number is larger than the last fx.\n",
    "You should seed your shuffling so you get the same one each time. Otherwise if you train an algorithm on the same set several times it will end up knowing the whole set.\n",
    "You also need to ensure you data is representative of the true data population, otherwise you should consider stratifying your dataset.\n",
    "\n",
    "#### Resume: Explore and Visualize the Data to Gain Insights,\n",
    "This segment is about data exploration and visualization to grain insight in the data. We have to get a more in depth about the data, which only do for the traing sets. It is a good idea to visualize the data through heatmaps/histograms. You may need to set the opacity down or color different segment for finding the patterns. You will also have to look at the correlations between the different value, but only if the dataset is not too large. The correlation coefficient ranges -1 to 1 and if the correlation is close to -1 or 1 it tells there is correlation between the data. Zero mean there is no linear correlation. The last part of this segment is about experiment with attribute combinations. Some atrributes maybe don't make sence they stand alone, so maybe combine them.\n",
    "\n",
    "#### Resume: Prepare the Data for Machine Learning Algorithms\n",
    "\n",
    "You need to find a strategy for handling missing datapoints such as entries missing some attributes. You can either remove them, use the median, or if you want to get fancy use the k-nearest neighbour.\n",
    "Furthermore machine learning models prefer numbers, and tend to skew towards numbers with larger intervals. Therfore it could be a good idea to standerdize your data to be between -1 - 1. This can be done by scaling all the \n",
    "\n",
    "#### Resume: Select and Train a Model\n",
    "\n",
    "You are now ready to select and train a model. Firstly try a simple model as linear regression, however some values may be way off. These data can be hard to understand, so we are going to use some tools for validation. This is can be MAE or RMSE. Another problem with the data if examples used the k-nearstnieghbor is overfitting or underfitting the training data. The main way to fix underfitting is try using a more complex model and how it proforms. The problem can also be the data is too small and one way to help simulate more date is it use the k-fold cross-validation, it spilt the data into nonoverlapping subsets. The model can now traing for every subsets and all the different score for each subsets put together give a value for its preformance. \n",
    "\n",
    "\n",
    "#### Resume: Fine-Tune Your Model\n",
    "\n",
    "After training the models, we have to fine tune the. For finetuning the model, you can manually try to find a great combination for the hyperparameter. Or use the GridSearchCV if you have a few hyperparameter or the RandomizedSearchCV for testing alot of hyperparameter. \n",
    "\n",
    "Another way to fine-tune your system is combine the models that performs best as a group. This mothod is called the Ensemble. It will often preform better the individual models. \n",
    "\n",
    "You may need to look into your best models and understand their errors. It can be some of the features don't make sense to have, so maybe you want to drop them. Or maybe it is because you are missing some features and need more data. May need to look into outliers.  \n",
    "\n",
    "When you system if preforming well on the train set, it may be time to try evaluating your system on the test set. After testing you will need to evalute the scores. The system may proferm really good on the test set or not. If it doesn't you may need to go back and finetune even more. If the system preform very well, then may need to make it ready for launch. \n",
    "\n",
    "\n",
    "\n",
    "#### Resume: Launch, Monitor, and Maintain Your System\n",
    "\n",
    "If your system came to this step, you will need to get it ready for production. For a system to go into it need to be launch, monitor and maintain. This can be done differenly ways. First way is that you can create documentation and conduct tests and integrate it. The alternative approach is using a model on cloud platform. \n",
    "\n",
    "It is not the last step to just deploying it. You will have to collect new fresh data and keep training the model and evaluating performance. It is import to catch potiontial issuses early. It is also a good idea to have a backup of the models in a database. The models can end up being corrupted. \n",
    "\n",
    "#### Resume: Try It Out!.\n",
    "\n",
    "Now you have read all the a both segment, you will have a understanding of what machine learning project look like. You can now see how much goes into making a grate system. The machine learning algorigthms are important, however it is preferable to be comfortable with the hole process.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
