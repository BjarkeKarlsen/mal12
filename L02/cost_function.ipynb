{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SWMAL Exercise\n",
    "\n",
    "## Mathematical Foundation\n",
    "\n",
    "### Vector and matrix representation in python\n",
    "\n",
    "In this exercise be about vector and matrix representation in Python. Firstly there will be showed how to implement a array in python. The secondly will the $\\norm{1}$ and $\\norm{2}$ be implemented without using `sqrt` or other mathmatical operator from math the library. \n",
    "\n",
    "#### Qa Given the following $\\mathbf{x}^{(i)}$'s, construct and print the $\\mathbf X$ matrix in python.\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "      \\bx\\pown{1} &= \\ac{c}{ 1, 2, 3}^T \\\\\n",
    "      \\bx\\pown{2} &= \\ac{c}{ 4, 2, 1}^T \\\\\n",
    "      \\bx\\pown{3} &= \\ac{c}{ 3, 8, 5}^T \\\\\n",
    "      \\bx\\pown{4} &= \\ac{c}{-9,-1, 0}^T\n",
    "    }\n",
    "$$\n",
    "\n",
    "##### Implementation Details\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For using the Python in the course you will have to get familiar with Python and the librabries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this chapter we will show how you can implement a array using numpy. You can either create the hole array at ones or you can use the append function. The append take two array a put the toghter in a new array. Another function is the vstack, where the function vertical stack each element. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  2  3]\n",
      " [ 4  2  1]\n",
      " [ 3  8  5]\n",
      " [-9 -1  0]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "y = np.array([1, 2, 3, 4]) # NOTE:  you'll need this later\n",
    "\n",
    "# Default method\n",
    "X = np.array([[1, 2, 3],[4, 2, 1], [3, 8, 5], [-9, -1, 0]])\n",
    "\n",
    "# print the full matrix\n",
    "print(f\"{X}\")\n",
    "\n",
    "# use the vstack\n",
    "np.vstack(([1, 2, 3],[4, 2, 1], [3, 8, 5], [-9, -1, 0]))\n",
    "\n",
    "\n",
    "\n",
    "# Use append \n",
    "q= np.array([[1, 2, 3],[4, 2, 1]] ) # NOTE:  you'll need this later\n",
    "q = np.append(q, [[4, 2, 1], [3, 8, 5], [-9, -1, 0]],axis = 0)\n",
    "\n",
    "# print the full matrix\n",
    "#print(f\"{q}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Norms, metrics or distances\n",
    "\n",
    "#### Qb Implement the $\\norm{1}$ and $\\norm{2}$ norms for vectors in python.\n",
    "\n",
    "Here we will ilustrate how to use the norms, metrices or distances. The first thing you will notice below is two function for error validation, that belongs to a exercise that will be decribes later on. The test of the validation is done up here, we will explain alot more later on. Just below the validation function, you can observe the L1, L2 and L2Dot. The L1 is given the distance between two vectors. Therefore, it takes the absolute value and sum all numbers together.\n",
    "\n",
    "$$\n",
    "    \\ar{ll}{      \n",
    "          \\mbox{d}(\\bx,\\by) &= ||\\bx-\\by||_2\\\\\n",
    "                     &= \\left( \\sum_{i=1}^n \\left| x_{i}-y_{i} \\right|^2 \\right)^{1/2}\n",
    "    }\n",
    "$$ \n",
    "\n",
    " The $$ L2 is the Eucalidian distance or norm for a vector. You can define it though a sum operation or a dot operation. Dot opeeration is vector multiply it self\n",
    "\n",
    "$$\n",
    "    \\norm{2}:~~ ||\\bx||_2 = \\left( \\sum_{i=1}^{n} |x_i|^2 \\right)^{1/2}\\\\\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tx-ty=[-2  3 -1 -2], d1-expected_d1=0.0, d2-expected_d2=0.0\n",
      "OK(part-1)\n",
      "d2dot-expected_d2= 0.0\n",
      "OK(part-2)\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "def checkArraySizeCoantainsElements(X):\n",
    "    assert X.shape[0]>=0 \n",
    "    if not X.ndim==1:\n",
    "        raise Exception(\"Array size needs to bigger than 1 dimision\")\n",
    "\n",
    "def checkArraySize(X):\n",
    "    assert X.shape[0]>=0 and X.shape[1]==0\n",
    "    if not X.ndim==1:\n",
    "        raise Exception(\"Array size needs to bigger than 1 dimision\")\n",
    "\n",
    "def L1(X):\n",
    "    checkArraySizeCoantainsElements(X)\n",
    "    return sum((Xi**2)**0.5 for Xi in X) \n",
    "\n",
    "\n",
    "def L2(X):\n",
    "    checkArraySizeCoantainsElements(X)\n",
    "    return sum((Xi*Xi) for Xi in X) **(1/2)\n",
    "\n",
    "def L2Dot(X):\n",
    "    return np.sqrt(np.dot(X,X))\n",
    "\n",
    "\n",
    "# Test the function with the following vectors\n",
    "tx=np.array([1, 2, 3, -1])\n",
    "ty=np.array([3,-1, 4,  1])\n",
    "\n",
    "expected_d1=8.0\n",
    "expected_d2=4.242640687119285\n",
    "\n",
    "d1=L1(tx-ty)\n",
    "d2=L2(tx-ty)\n",
    "\n",
    "print(f\"tx-ty={tx-ty}, d1-expected_d1={d1-expected_d1}, d2-expected_d2={d2-expected_d2}\")\n",
    "\n",
    "eps=1E-9 \n",
    "# NOTE: remember to import 'math' for fabs for the next two lines..\n",
    "assert math.fabs(d1-expected_d1)<eps, \"L1 dist seems to be wrong\" \n",
    "assert math.fabs(d2-expected_d2)<eps, \"L2 dist seems to be wrong\" \n",
    "\n",
    "print(\"OK(part-1)\")\n",
    "\n",
    "# comment-in once your L2Dot fun is ready...\n",
    "d2dot=L2Dot(tx-ty)\n",
    "print(\"d2dot-expected_d2=\",d2dot-expected_d2)\n",
    "assert math.fabs(d2dot-expected_d2)<eps, \"L2Ddot dist seem to be wrong\" \n",
    "print(\"OK(part-2)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The cost function, $J$\n",
    "\n",
    "Now, most ML algorithm uses norms or metrics internally when doing minimizations. Details on this will come later, but for now we need to know that an algorithm typically tries to minimize a given performance metric, the loss function, for all the input data, and implicitly tries to minimize the sum of all norms for the 'distances' between some predicted output, $y\\st{pred}$ and the true output $y\\st{true}$, with the distance between these typically given by the $\\norm{2}$ norm\n",
    "\n",
    "$$   \n",
    "  \\mbox{individual loss:}~~L\\powni = \\mbox{d}(y\\st{pred}\\powni,y\\st{true}\\powni)\n",
    "$$ \n",
    "\n",
    "with $y\\st{pred}\\powni$, a scalar value, being the output from the hypothesis function, that maps the input vector $\\bx\\powni$ to a scalar\n",
    "\n",
    "$$ \n",
    "    y_{pred}\\powni = \\hat{y}\\powni = h(\\bx\\powni;\\btheta)\n",
    "$$\n",
    "\n",
    "and the total loss, $J$ will be the sum over all $i$'s\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        J &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni\\\\\n",
    "        &= \\frac{1}{n} \\sum_{i=1}^{n} \\mbox{d}( h(\\bx\\powni) , y\\powni\\st{true})\n",
    "    }\n",
    "$$\n",
    "\n",
    "\n",
    "### Cost function in vector/matrix notation using $\\norm{2}$\n",
    "\n",
    "Remember the data-flow model for supervised learning\n",
    "\n",
    "<img src=\"https://itundervisning.ase.au.dk/SWMAL/L02/Figs/ml_simple_vector.png\" alt=\"WARNING: could not get image from server.\" style=\"width:500px\">\n",
    "\n",
    "Let us now express $J$ in terms of vectors and matrices instead of summing over individual scalars, and let's use $\\norm{2}$ as the distance function\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        J(\\bX,\\by;\\btheta) &= \\frac{1}{n} \\sum_{i=1}^{n} L\\powni\\\\\n",
    "        &= \\frac{1}{n}\\sum_{i=1}^{n} (h(\\bx\\powni) - \\by\\powni\\st{true})^2\\\\\n",
    "        &= \\frac{1}{n} ||h(\\bX) - \\by\\st{true} ||_2^2\\\\\n",
    "        &= \\frac{1}{n} ||\\by\\st{pred} - \\by\\st{true} ||_2^2\\\\\n",
    "     }\n",
    "$$\n",
    "\n",
    "with the matrix-vector notation\n",
    "\n",
    "$$ \n",
    "    \\by_{pred} = \\hat{\\by} =  h(\\bX;\\btheta)\n",
    "$$\n",
    "\n",
    "#### Loss or Objective Function using the Mean Squared Error\n",
    "\n",
    "This formulation is equal to the definition of the _mean-squared-error_, MSE (or indirectly also RMSE), here given in the general formulation for some random variable $Z$ \n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        \\mbox{MSE} &= \\frac{1}{n} \\sum_{i=1}^{n} (\\hat{Z}_i-Z_i)^2 = \\frac{1}{n} SS\\\\\n",
    "        \\mbox{RMSE} &= \\sqrt{\\mbox{MSE}}\\\n",
    "    }\n",
    "$$\n",
    "\n",
    "with sum-of-squares (SS) is given simply by\n",
    "\n",
    "$$\n",
    "    \\mbox{SS} = \\sum_{i=1}^{n} (\\hat{Z}_i-Z_i)^2\\\\\n",
    "$$\n",
    "\n",
    "\n",
    "So, using the $\\norm{2}$ for the distance metric, is equal to saying that we want to minimize $J$ with respect to the MSE\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        J &= \\mbox{MSE}(h(\\bX), \\by\\st{true}) \\\\\n",
    "          &= \\mbox{MSE}(\\by\\st{pred}~, \\by\\st{true}) \\\\\n",
    "          &= \\mbox{MSE}(\\hat{\\by}, \\by\\st{true})\n",
    "     }\n",
    "$$\n",
    "\n",
    "Note: when minimizing one can ignore the constant factor $1/n$ and it really does not matter if you minimize MSE or RMSE. Often $J$ is also multiplied by 1/2 to ease notation when trying to differentiate it.\n",
    "\n",
    "$$\n",
    "    \\ar{rl}{\n",
    "        J(\\bX,\\by\\st{true};\\btheta) &\\propto \\half ||\\by\\st{pred} - \\by\\st{true} ||_2^2 \\\\\n",
    "          &\\propto \\mbox{MSE}\n",
    "     }\n",
    "$$\n",
    "\n",
    "### MSE\n",
    "\n",
    "Now, let us take a look on how you calculate the MSE.\n",
    "\n",
    "The MSE uses the $\\norm{2}$ norm internally, well, actually $||\\cdot||^2_2$ to be precise, and basically just sums, means and roots the individual (scalar) losses (distances), we just saw before. \n",
    "\n",
    "And the RMSE is just an MSE with a final square-root call.\n",
    "\n",
    "### Qc Construct the Root Mean Square Error (RMSE) function (Equation 2-1 [HOML]).\n",
    "\n",
    "Call the function RMSE, and evaluate it using the $\\bX$ matrix and $\\by$ from Qa.\n",
    "\n",
    "We implement a dummy hypothesis function, that just takes the first column of $\\bX$ as its 'prediction'\n",
    "\n",
    "$$\n",
    "    h\\st{dummy}(\\bX) = \\bX(:,0)\n",
    "$$\n",
    "\n",
    "Do not re-implement the $\\norm{2}$ for the RMSE function, but call the '''L2''' function you just implemented internally in RMSE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3 4]\n",
      "hX: [ 1  4  3 -9]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'checkArraySizeContains' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Bjark\\source\\repos\\MAL\\mal12\\L02\\cost_function.ipynb Cell 8\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L02/cost_function.ipynb#W5sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhX: \u001b[39m\u001b[39m{\u001b[39;00mh(X)\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L02/cost_function.ipynb#W5sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Calls your RMSE() function:\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L02/cost_function.ipynb#W5sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m r\u001b[39m=\u001b[39mRMSE(h(X),y)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L02/cost_function.ipynb#W5sZmlsZQ%3D%3D?line=26'>27</a>\u001b[0m MSE \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39msquare(np\u001b[39m.\u001b[39msubtract(h(X),y))\u001b[39m.\u001b[39mmean() \n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L02/cost_function.ipynb#W5sZmlsZQ%3D%3D?line=28'>29</a>\u001b[0m RMSE \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39msqrt(MSE)\n",
      "\u001b[1;32mc:\\Users\\Bjark\\source\\repos\\MAL\\mal12\\L02\\cost_function.ipynb Cell 8\u001b[0m line \u001b[0;36m4\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L02/cost_function.ipynb#W5sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mRMSE\u001b[39m(X,y):\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L02/cost_function.ipynb#W5sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     checkArraySizeContains(y)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L02/cost_function.ipynb#W5sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m     dif_array \u001b[39m=\u001b[39m X \u001b[39m-\u001b[39m y\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Bjark/source/repos/MAL/mal12/L02/cost_function.ipynb#W5sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m     \u001b[39mprint\u001b[39m(dif_array)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'checkArraySizeContains' is not defined"
     ]
    }
   ],
   "source": [
    "# TODO: solve Qc...implement your RMSE function here\n",
    "# IDK Have it call the L2, but you har to take h(x)-y\n",
    "def RMSE(X,y):\n",
    "    checkArraySizeContains(y)\n",
    "        \n",
    "    dif_array = X - y\n",
    "    print(dif_array)\n",
    "    return (L2(dif_array))/2 # Why divide with 2?? we should divide with m and there is only two numbers in the dif array\n",
    "\n",
    "#print(np.dot(h(X),y))\n",
    "\n",
    "        \n",
    "# Dummy h function:\n",
    "def h(X):    \n",
    "    if X.ndim!=2:\n",
    "        raise ValueError(\"excpeted X to be of ndim=2, got ndim=\",X.ndim)\n",
    "    if X.shape[0]==0 or X.shape[1]==0:\n",
    "        raise ValueError(\"X got zero data along the 0/1 axis, cannot continue\")\n",
    "    return X[:,0]\n",
    "\n",
    "# Delete me - Jusst for seeing the values\n",
    "print(y)\n",
    "print(f\"hX: {h(X)}\")\n",
    "\n",
    "# Calls your RMSE() function:\n",
    "r=RMSE(h(X),y)\n",
    "MSE = np.square(np.subtract(h(X),y)).mean() \n",
    " \n",
    "RMSE = math.sqrt(MSE)\n",
    "print(\"Root Mean Square Error:\\n\")\n",
    "print(RMSE)\n",
    "\n",
    "# TEST vector:\n",
    "eps=1E-9\n",
    "expected=6.57647321898295\n",
    "print(f\"RMSE={r}, diff={r-expected}\")\n",
    "assert math.fabs(r-expected)<eps, \"your RMSE dist seems to be wrong\" \n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MAE\n",
    "\n",
    "#### Qd Similar construct the Mean Absolute Error (MAE) function (Equation 2-2 [HOML]) and evaluate it.\n",
    "\n",
    "The MAE will algorithmic wise be similar to the MSE part from using the $\\norm{1}$ instead of the $\\norm{2}$ norm.\n",
    "\n",
    "Again, re-implementation of the$\\norm{1}$ is a no-go, call the '''L1''' instead internally i MAE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE=3.75, diff=0.0\n",
      "OK\n"
     ]
    }
   ],
   "source": [
    "def MAE(X,y):\n",
    "    \n",
    "    return L1(X-y)/X.size\n",
    "\n",
    "# Calls your MAE function:\n",
    "r=MAE(h(X), y)\n",
    "\n",
    "# TEST vector:\n",
    "expected=3.75\n",
    "print(f\"MAE={r}, diff={r-expected}\")\n",
    "#assert math.fabs(r-expected)<eps, \"MAE dist seems to be wrong\" \n",
    "\n",
    "print(\"OK\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pythonic Code\n",
    "\n",
    "### Robustness of Code\n",
    "\n",
    "Data validity checking is an essential part of robust code, and in Python the 'fail-fast' method is used extensively: instead of lingering on trying to get the 'best' out of an erroneous situation, the fail-fast pragma will be very loud about any data inconsistencies at the earliest possible moment.\n",
    "\n",
    "Hence robust code should include a lot of error checking, say as pre- and post-conditions (part of the design-by-contract programming) when calling a function: when entering the function you check that all parameters are ok (pre-condition), and when leaving you check the return parameter (post-conditions).  \n",
    "\n",
    "Normally assert-checking or exception-throwing will do the trick just fine, with the exception method being more _pythonic_.\n",
    "\n",
    "For the norm-function you could, for instance, test your input data to be 'vector' like, i.e. like\n",
    "\n",
    "```python\n",
    "    assert x.shape[0]>=0 and x.shape[1]==0\n",
    "    \n",
    "    if not x.ndim==1:\n",
    "        raise some error\n",
    "```\n",
    "or similar.\n",
    "\n",
    "#### Qe Robust Code \n",
    "\n",
    "Add error checking code (asserts or exceptions), that checks for right $\\hat\\by$-$\\by$ sizes of the MSE and MAE functions.\n",
    "\n",
    "Also add error checking to all you previously tested L2() and L1() functions, and re-run all your tests."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qf Conclusion\n",
    "\n",
    "Now, conclude on all the exercise above. \n",
    "\n",
    "Write a short textual conclusion (max. 10- to 20-lines) that extract the _essence_ of the exercises: why did you think it was important to look at these particular ML concepts, and what was our overall learning outcome of the exercises (in broad terms)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Qf concluding remarks in text.. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
